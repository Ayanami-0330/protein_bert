{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS_DIR = '/home/nemophila/projects/protein_bert/anticrispr_benchmarks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 12:28:56.960820: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:56.960842: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996 training set records, 111 validation set records, 286 test set records.\n",
      "[2026_02_05-12:28:58] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
      "[2026_02_05-12:28:58] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
      "[2026_02_05-12:28:58] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 12:28:58.242530: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2026-02-05 12:28:58.243363: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2026-02-05 12:28:58.249904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:ab:00.0 name: NVIDIA L40S computeCapability: 8.9\n",
      "coreClock: 2.52GHz coreCount: 142 deviceMemorySize: 44.53GiB deviceMemoryBandwidth: 804.75GiB/s\n",
      "2026-02-05 12:28:58.249979: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250020: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250057: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250093: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2026-02-05 12:28:58.250459: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250493: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250531: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2026-02-05 12:28:58.250536: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2026-02-05 12:28:58.250959: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-05 12:28:58.263776: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2026-02-05 12:28:58.263799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2026-02-05 12:28:58.263805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2026-02-05 12:28:59.389973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2026-02-05 12:28:59.390378: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "32/32 [==============================] - 18s 454ms/step - loss: 0.5095 - val_loss: 0.3420\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 12s 381ms/step - loss: 0.2953 - val_loss: 0.3220\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 14s 423ms/step - loss: 0.2599 - val_loss: 0.3303\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 13s 404ms/step - loss: 0.2302 - val_loss: 0.3340\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "[2026_02_05-12:29:56] Training the entire fine-tuned model...\n",
      "[2026_02_05-12:30:03] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 30s 795ms/step - loss: 0.2479 - val_loss: 0.3108\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 21s 665ms/step - loss: 0.2136 - val_loss: 0.3116\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 20s 638ms/step - loss: 0.1940 - val_loss: 0.3102\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 21s 657ms/step - loss: 0.1844 - val_loss: 0.3124\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 21s 665ms/step - loss: 0.1709 - val_loss: 0.3123\n",
      "[2026_02_05-12:31:58] Training on final epochs of sequence length 1024...\n",
      "[2026_02_05-12:31:58] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
      "[2026_02_05-12:31:58] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
      "63/63 [==============================] - 53s 770ms/step - loss: 0.2164 - val_loss: 0.3284\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>286</td>\n",
       "      <td>0.884024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>286</td>\n",
       "      <td>0.884024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                  286  0.884024\n",
       "All                  286  0.884024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "0  256  4\n",
       "1   17  9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "# ===================== 1. 修改基准名称（对应你的数据集前缀） =====================\n",
    "BENCHMARK_NAME = 'anticrispr_binary'  # 替换原signalP_binary为你的数据集前缀\n",
    "\n",
    "# A local (non-global) binary output\n",
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]  # 你的数据集也是二分类（0/1），无需修改\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "# ===================== 2. 定义你的数据集根目录（核心修改） =====================\n",
    "# 替换原BENCHMARKS_DIR，指向你的anticrispr_benchmarks文件夹绝对路径\n",
    "BENCHMARKS_DIR = '/home/nemophila/projects/protein_bert/anticrispr_benchmarks'\n",
    "\n",
    "# Loading the dataset\n",
    "# ===================== 3. 加载你自己的训练/测试集（路径适配） =====================\n",
    "# 加载训练集（你的anticrispr_binary.train.csv）\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "# 从训练集中拆分验证集（和原逻辑一致，按标签分层拆分）\n",
    "train_set, valid_set = train_test_split(train_set, stratify = train_set['label'], test_size = 0.1, random_state = 0)\n",
    "\n",
    "# 加载测试集（你的anticrispr_binary.test.csv）\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "# 打印数据集大小（验证是否加载成功）\n",
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n",
    "\n",
    "# ===================== 以下部分无需修改（模型训练/评估逻辑通用） =====================\n",
    "# Loading the pre-trained model and fine-tuning it on the loaded dataset\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "# get_model_with_hidden_layers_as_outputs gives the model output access to the hidden layers (on top of the output)\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
    "]\n",
    "\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
    "        seq_len = 512, batch_size = 32, max_epochs_per_stage = 40, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)\n",
    "\n",
    "# Evaluating the performance on the test-set\n",
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'], \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "\n",
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026_02_05-14:13:09] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
      "[2026_02_05-14:13:09] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
      "[2026_02_05-14:13:09] Test set: Filtered out 0 of 286 (0.0%) records of lengths exceeding 510.\n",
      "Debug: manual feature dim: 230\n",
      "Debug: cross-attention layer: True\n",
      "Debug: seq kv dim: (None, 512, 1562)\n",
      "Debug: global dim: (None, 15599)\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 17s 329ms/step - loss: 0.3525 - val_loss: 0.2640\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 9s 280ms/step - loss: 0.1868 - val_loss: 0.1303\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 9s 287ms/step - loss: 0.1483 - val_loss: 0.4441\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "Epoch 4/40\n",
      "32/32 [==============================] - 9s 281ms/step - loss: 0.1698 - val_loss: 0.1245\n",
      "Epoch 5/40\n",
      "32/32 [==============================] - 9s 285ms/step - loss: 0.0813 - val_loss: 0.1310\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "Epoch 6/40\n",
      "32/32 [==============================] - 9s 289ms/step - loss: 0.0559 - val_loss: 0.1038\n",
      "Epoch 7/40\n",
      "32/32 [==============================] - 9s 292ms/step - loss: 0.0262 - val_loss: 0.1092\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "Epoch 8/40\n",
      "32/32 [==============================] - 9s 283ms/step - loss: 0.0250 - val_loss: 0.1043\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "Epoch 1/40\n",
      "32/32 [==============================] - 29s 715ms/step - loss: 0.0347 - val_loss: 0.0827\n",
      "Epoch 2/40\n",
      "32/32 [==============================] - 21s 665ms/step - loss: 0.0242 - val_loss: 0.0894\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 3/40\n",
      "32/32 [==============================] - 21s 663ms/step - loss: 0.0157 - val_loss: 0.0982\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "[2026_02_05-14:15:43] Training set (final): Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
      "[2026_02_05-14:15:43] Validation set (final): Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
      "63/63 [==============================] - 50s 697ms/step - loss: 0.0360 - val_loss: 0.0871\n",
      "Debug: best threshold: 0.39999999999999997 AUPRC(valid): 0.6331 P: 0.5556 R: 0.7143 F1: 0.625\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AUPRC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>286</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.619707</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>286</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.619707</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC     AUPRC        F1\n",
       "Model seq len                                         \n",
       "512                  286  0.905473  0.619707  0.521739\n",
       "All                  286  0.905473  0.619707  0.521739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>235</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  235  25\n",
       "1    8  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from proteinbert.finetuning import filter_dataset_by_len, split_dataset_by_len\n",
    "from proteinbert.feature_extraction import extract_features\n",
    "from proteinbert.cross_attention_head import build_cross_attention_model\n",
    "\n",
    "def compute_sample_weights(labels):\n",
    "    labels = np.array(labels, dtype = int)\n",
    "    counts = np.bincount(labels, minlength = 2)\n",
    "    total = counts.sum()\n",
    "    weights = total / (2.0 * np.maximum(counts, 1))\n",
    "    return np.array([weights[label] for label in labels], dtype = np.float32)\n",
    "\n",
    "def focal_loss(alpha = 0.75, gamma = 2.0):\n",
    "    def _loss(y_true, y_pred):\n",
    "        y_true = keras.backend.cast(y_true, \"float32\")\n",
    "        y_pred = keras.backend.clip(y_pred, keras.backend.epsilon(), 1.0 - keras.backend.epsilon())\n",
    "        ce = -(y_true * keras.backend.log(y_pred) + (1.0 - y_true) * keras.backend.log(1.0 - y_pred))\n",
    "        p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n",
    "        alpha_t = y_true * alpha + (1.0 - y_true) * (1.0 - alpha)\n",
    "        return keras.backend.mean(alpha_t * keras.backend.pow(1.0 - p_t, gamma) * ce)\n",
    "    return _loss\n",
    "\n",
    "def filter_with_features(df, features, seq_len, name):\n",
    "    df = df.copy()\n",
    "    df[\"__idx\"] = np.arange(len(df))\n",
    "    filtered = filter_dataset_by_len(df, seq_len = seq_len, dataset_name = name, verbose = True)\n",
    "    feat = features[filtered[\"__idx\"].values]\n",
    "    filtered = filtered.drop(columns = [\"__idx\"]).reset_index(drop = True)\n",
    "    return filtered, feat\n",
    "\n",
    "def train_cross_attention(train_df, valid_df, train_features, valid_features, seq_len = 512, batch_size = 32, max_epochs = 40,\n",
    "        begin_with_frozen = True, lr_frozen = 1e-02, lr = 1e-04, n_heads = 4, key_dim = 64,\n",
    "        final_seq_len = 1024, n_final_epochs = 1, final_lr = 1e-05):\n",
    "    feature_dim = train_features.shape[1]\n",
    "    model, base_model = build_cross_attention_model(pretrained_model_generator, seq_len, feature_dim, n_heads = n_heads, key_dim = key_dim, dropout_rate = 0.5)\n",
    "    X_train = input_encoder.encode_X(train_df[\"seq\"], seq_len)\n",
    "    X_valid = input_encoder.encode_X(valid_df[\"seq\"], seq_len)\n",
    "    y_train = train_df[\"label\"].values.astype(float)\n",
    "    y_valid = valid_df[\"label\"].values.astype(float)\n",
    "    sw_train = compute_sample_weights(y_train)\n",
    "    sw_valid = compute_sample_weights(y_valid)\n",
    "    loss_fn = focal_loss(alpha = 0.75, gamma = 2.0)\n",
    "    if begin_with_frozen:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr_frozen), loss = loss_fn)\n",
    "        model.fit(X_train + [train_features], y_train, sample_weight = sw_train, validation_data = (X_valid + [valid_features], y_valid, sw_valid),\n",
    "                batch_size = batch_size, epochs = max_epochs, callbacks = training_callbacks)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss = loss_fn)\n",
    "    model.fit(X_train + [train_features], y_train, sample_weight = sw_train, validation_data = (X_valid + [valid_features], y_valid, sw_valid),\n",
    "            batch_size = batch_size, epochs = max_epochs, callbacks = training_callbacks)\n",
    "    if n_final_epochs > 0:\n",
    "        final_batch_size = max(int(batch_size / (final_seq_len / seq_len)), 1)\n",
    "        train_f, train_feat_f = filter_with_features(train_df, train_features, seq_len = final_seq_len, name = \"Training set (final)\")\n",
    "        valid_f, valid_feat_f = filter_with_features(valid_df, valid_features, seq_len = final_seq_len, name = \"Validation set (final)\")\n",
    "        X_train_f = input_encoder.encode_X(train_f[\"seq\"], final_seq_len)\n",
    "        X_valid_f = input_encoder.encode_X(valid_f[\"seq\"], final_seq_len)\n",
    "        y_train_f = train_f[\"label\"].values.astype(float)\n",
    "        y_valid_f = valid_f[\"label\"].values.astype(float)\n",
    "        sw_train_f = compute_sample_weights(y_train_f)\n",
    "        sw_valid_f = compute_sample_weights(y_valid_f)\n",
    "        final_model, final_base = build_cross_attention_model(pretrained_model_generator, final_seq_len, feature_dim, n_heads = n_heads, key_dim = key_dim, dropout_rate = 0.5)\n",
    "        final_model.set_weights(model.get_weights())\n",
    "        for layer in final_base.layers:\n",
    "            layer.trainable = True\n",
    "        final_model.compile(optimizer = keras.optimizers.Adam(learning_rate = final_lr), loss = loss_fn)\n",
    "        final_model.fit(X_train_f + [train_feat_f], y_train_f, sample_weight = sw_train_f, validation_data = (X_valid_f + [valid_feat_f], y_valid_f, sw_valid_f),\n",
    "                batch_size = final_batch_size, epochs = n_final_epochs, callbacks = training_callbacks)\n",
    "        model = final_model\n",
    "    return model\n",
    "\n",
    "def _collect_metrics(y_true, y_pred, y_pred_class):\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "        auprc = average_precision_score(y_true, y_pred)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "        auprc = np.nan\n",
    "    f1 = f1_score(y_true, y_pred_class, zero_division = 0)\n",
    "    return {\"# records\": len(y_true), \"AUC\": auc, \"AUPRC\": auprc, \"F1\": f1}\n",
    "\n",
    "def _predict_by_len(model_weights, df, features, start_seq_len = 512, start_batch_size = 32, increase_factor = 2, n_heads = 6, key_dim = 64):\n",
    "    dataset = df.copy()\n",
    "    dataset[\"idx\"] = np.arange(len(dataset))\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = start_seq_len, start_batch_size = start_batch_size,\n",
    "            increase_factor = increase_factor):\n",
    "        if len(len_matching_dataset) == 0:\n",
    "            continue\n",
    "        idx = len_matching_dataset[\"idx\"].values\n",
    "        feats = features[idx]\n",
    "        X = input_encoder.encode_X(len_matching_dataset[\"seq\"], seq_len)\n",
    "        model, _ = build_cross_attention_model(pretrained_model_generator, seq_len, feats.shape[1], n_heads = n_heads, key_dim = key_dim, dropout_rate = 0.5)\n",
    "        model.set_weights(model_weights)\n",
    "        y_true = len_matching_dataset[\"label\"].values.astype(int)\n",
    "        y_pred = model.predict(X + [feats], batch_size = batch_size).flatten()\n",
    "        all_true.append(y_true)\n",
    "        all_pred.append(y_pred)\n",
    "    return np.concatenate(all_true, axis = 0), np.concatenate(all_pred, axis = 0)\n",
    "\n",
    "def find_best_threshold(valid_df, valid_features, model_weights, start_seq_len = 512, start_batch_size = 32, min_recall = 0.6):\n",
    "    \"\"\"Precision优先策略：在满足min_recall约束的阈值中，选择Precision最高的\"\"\"\n",
    "    y_true, y_pred = _predict_by_len(model_weights, valid_df, valid_features, start_seq_len = start_seq_len, start_batch_size = start_batch_size)\n",
    "    best = {\"thr\": 0.5, \"f1\": -1.0, \"precision\": 0.0, \"recall\": 0.0, \"auprc\": average_precision_score(y_true, y_pred)}\n",
    "    for thr in np.linspace(0.05, 0.95, 19):\n",
    "        y_pred_class = (y_pred >= thr).astype(int)\n",
    "        precision = precision_score(y_true, y_pred_class, zero_division = 0)\n",
    "        recall = recall_score(y_true, y_pred_class, zero_division = 0)\n",
    "        f1 = f1_score(y_true, y_pred_class, zero_division = 0)\n",
    "        # 满足最小recall约束，优先最大化precision（减少FP）\n",
    "        if recall >= min_recall and precision > best[\"precision\"]:\n",
    "            best = {\"thr\": thr, \"f1\": f1, \"precision\": precision, \"recall\": recall, \"auprc\": average_precision_score(y_true, y_pred)}\n",
    "    return best\n",
    "\n",
    "def evaluate_by_len_custom(model_weights, df, features, start_seq_len = 512, start_batch_size = 32, increase_factor = 2, threshold = 0.5, n_heads = 6, key_dim = 64):\n",
    "    dataset = df.copy()\n",
    "    dataset[\"idx\"] = np.arange(len(dataset))\n",
    "    results = []\n",
    "    results_names = []\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = start_seq_len, start_batch_size = start_batch_size,\n",
    "            increase_factor = increase_factor):\n",
    "        if len(len_matching_dataset) == 0:\n",
    "            continue\n",
    "        idx = len_matching_dataset[\"idx\"].values\n",
    "        feats = features[idx]\n",
    "        X = input_encoder.encode_X(len_matching_dataset[\"seq\"], seq_len)\n",
    "        model, _ = build_cross_attention_model(pretrained_model_generator, seq_len, feats.shape[1], n_heads = n_heads, key_dim = key_dim, dropout_rate = 0.5)\n",
    "        model.set_weights(model_weights)\n",
    "        y_true = len_matching_dataset[\"label\"].values.astype(int)\n",
    "        y_pred = model.predict(X + [feats], batch_size = batch_size).flatten()\n",
    "        y_pred_class = (y_pred >= threshold).astype(int)\n",
    "        results.append(_collect_metrics(y_true, y_pred, y_pred_class))\n",
    "        results_names.append(seq_len)\n",
    "        all_true.append(y_true)\n",
    "        all_pred.append(y_pred)\n",
    "    y_true = np.concatenate(all_true, axis = 0)\n",
    "    y_pred = np.concatenate(all_pred, axis = 0)\n",
    "    y_pred_class = (y_pred >= threshold).astype(int)\n",
    "    all_results = _collect_metrics(y_true, y_pred, y_pred_class)\n",
    "    cm = confusion_matrix(y_true, y_pred_class, labels = [0, 1])\n",
    "    results.append(all_results)\n",
    "    results_names.append(\"All\")\n",
    "    results_df = pd.DataFrame(results, index = results_names)\n",
    "    results_df.index.name = \"Model seq len\"\n",
    "    cm_df = pd.DataFrame(cm, index = [\"0\", \"1\"], columns = [\"0\", \"1\"])\n",
    "    return results_df, cm_df\n",
    "\n",
    "train_features = extract_features(train_set[\"seq\"], nc_len = 20, paac_lambda = 3)\n",
    "valid_features = extract_features(valid_set[\"seq\"], nc_len = 20, paac_lambda = 3)\n",
    "test_features = extract_features(test_set[\"seq\"], nc_len = 20, paac_lambda = 3)\n",
    "\n",
    "train_filtered, train_features_f = filter_with_features(train_set, train_features, seq_len = 512, name = \"Training set\")\n",
    "valid_filtered, valid_features_f = filter_with_features(valid_set, valid_features, seq_len = 512, name = \"Validation set\")\n",
    "test_filtered, test_features_f = filter_with_features(test_set, test_features, seq_len = 512, name = \"Test set\")\n",
    "\n",
    "print(\"Debug: manual feature dim:\", train_features_f.shape[1])\n",
    "_debug_model, _debug_base = build_cross_attention_model(pretrained_model_generator, 512, train_features_f.shape[1], dropout_rate = 0.5)\n",
    "print(\"Debug: cross-attention layer:\", any(layer.name == \"cross-attention\" for layer in _debug_model.layers))\n",
    "print(\"Debug: seq kv dim:\", _debug_base.output[0].shape)\n",
    "print(\"Debug: global dim:\", _debug_base.output[1].shape)\n",
    "\n",
    "cross_attn_model = train_cross_attention(train_filtered, valid_filtered, train_features_f, valid_features_f, seq_len = 512, batch_size = 32, max_epochs = 40,\n",
    "        begin_with_frozen = True, lr_frozen = 1e-02, lr = 1e-04, n_heads = 6, key_dim = 64, final_seq_len = 1024, n_final_epochs = 1, final_lr = 1e-05)\n",
    "\n",
    "model_weights = cross_attn_model.get_weights()\n",
    "best = find_best_threshold(valid_filtered, valid_features_f, model_weights, start_seq_len = 512, start_batch_size = 32, min_recall = 0.6)\n",
    "print(\"Debug: best threshold:\", best[\"thr\"], \"AUPRC(valid):\", round(best[\"auprc\"], 4),\n",
    "      \"P:\", round(best[\"precision\"], 4), \"R:\", round(best[\"recall\"], 4), \"F1:\", round(best[\"f1\"], 4))\n",
    "\n",
    "results, confusion_matrix_df = evaluate_by_len_custom(model_weights, test_set[[\"seq\", \"label\"]].copy(), test_features, start_seq_len = 512, start_batch_size = 32, threshold = best[\"thr\"])\n",
    "\n",
    "print(\"Test-set performance:\")\n",
    "display(results)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "display(confusion_matrix_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24pb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
