{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# Cell 0：全局路径配置\n",
        "# 作用：定义 Anti-CRISPR 数据集目录，供后续所有实验复用\n",
        "# =====================================================================\n",
        "BENCHMARKS_DIR = '/home/nemophila/projects/protein_bert/anticrispr_benchmarks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 12:25:21.955062: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:21.955095: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "996 training set records, 111 validation set records, 286 test set records.\n",
            "[2026_02_12-12:25:23] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:25:23] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:25:23] Training with frozen pretrained layers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 12:25:23.535198: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2026-02-12 12:25:23.536420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2026-02-12 12:25:23.557098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:ab:00.0 name: NVIDIA L40S computeCapability: 8.9\n",
            "coreClock: 2.52GHz coreCount: 142 deviceMemorySize: 44.53GiB deviceMemoryBandwidth: 804.75GiB/s\n",
            "2026-02-12 12:25:23.557191: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557237: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557277: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557317: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2026-02-12 12:25:23.557739: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557777: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557815: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2026-02-12 12:25:23.557820: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2026-02-12 12:25:23.558307: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-02-12 12:25:23.567888: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2026-02-12 12:25:23.567916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2026-02-12 12:25:23.567922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
            "2026-02-12 12:25:24.792167: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2026-02-12 12:25:24.792700: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500000000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "32/32 [==============================] - 14s 292ms/step - loss: 0.4515 - val_loss: 0.3429\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 8s 256ms/step - loss: 0.2993 - val_loss: 0.3457\n",
            "\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 8s 264ms/step - loss: 0.2398 - val_loss: 0.3301\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 8s 245ms/step - loss: 0.2461 - val_loss: 0.3351\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 8s 253ms/step - loss: 0.2128 - val_loss: 0.3326\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
            "[2026_02_12-12:26:11] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:26:17] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 27s 681ms/step - loss: 0.2340 - val_loss: 0.3333\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 20s 613ms/step - loss: 0.2080 - val_loss: 0.3315\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 19s 600ms/step - loss: 0.1690 - val_loss: 0.3607\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 19s 600ms/step - loss: 0.1274 - val_loss: 0.3473\n",
            "\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "[2026_02_12-12:27:43] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:27:43] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:27:43] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 44s 635ms/step - loss: 0.2030 - val_loss: 0.3355\n",
            "Test-set performance:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># records</th>\n",
              "      <th>AUC</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model seq len</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>286</td>\n",
              "      <td>0.89142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <td>286</td>\n",
              "      <td>0.89142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               # records      AUC\n",
              "Model seq len                    \n",
              "512                  286  0.89142\n",
              "All                  286  0.89142"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>257</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0   1\n",
              "0  257   3\n",
              "1   16  10"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# Cell 1：ProteinBERT 基线实验（原始微调流程）\n",
        "# 作用：作为对照基线，评估不加额外融合策略时的性能\n",
        "# =====================================================================\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
        "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
        "\n",
        "# ===================== 1. 修改基准名称（对应你的数据集前缀） =====================\n",
        "BENCHMARK_NAME = 'anticrispr_binary'  # 替换原signalP_binary为你的数据集前缀\n",
        "\n",
        "# A local (non-global) binary output\n",
        "OUTPUT_TYPE = OutputType(False, 'binary')\n",
        "UNIQUE_LABELS = [0, 1]  # 你的数据集也是二分类（0/1），无需修改\n",
        "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
        "\n",
        "# ===================== 2. 定义你的数据集根目录（核心修改） =====================\n",
        "# 替换原BENCHMARKS_DIR，指向你的anticrispr_benchmarks文件夹绝对路径\n",
        "BENCHMARKS_DIR = '/home/nemophila/projects/protein_bert/anticrispr_benchmarks'\n",
        "\n",
        "# Loading the dataset\n",
        "# ===================== 3. 加载你自己的训练/测试集（路径适配） =====================\n",
        "# 加载训练集（你的anticrispr_binary.train.csv）\n",
        "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
        "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
        "# 从训练集中拆分验证集（和原逻辑一致，按标签分层拆分）\n",
        "train_set, valid_set = train_test_split(train_set, stratify = train_set['label'], test_size = 0.1, random_state = 0)\n",
        "\n",
        "# 加载测试集（你的anticrispr_binary.test.csv）\n",
        "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
        "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
        "\n",
        "# 打印数据集大小（验证是否加载成功）\n",
        "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n",
        "\n",
        "# ===================== 以下部分无需修改（模型训练/评估逻辑通用） =====================\n",
        "# Loading the pre-trained model and fine-tuning it on the loaded dataset\n",
        "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
        "\n",
        "# get_model_with_hidden_layers_as_outputs gives the model output access to the hidden layers (on top of the output)\n",
        "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
        "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
        "\n",
        "training_callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
        "    keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
        "]\n",
        "\n",
        "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
        "        seq_len = 512, batch_size = 32, max_epochs_per_stage = 40, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
        "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)\n",
        "\n",
        "# Evaluating the performance on the test-set\n",
        "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'], \\\n",
        "        start_seq_len = 512, start_batch_size = 32)\n",
        "\n",
        "print('Test-set performance:')\n",
        "display(results)\n",
        "\n",
        "print('Confusion matrix:')\n",
        "display(confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Protocol] Train: 1107 (205+/902-)\n",
            "[Protocol] Test : 286 (26+/260-)\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验2：统一评估协议与微调工具函数（重构主线基础设施）\n",
        "# 目标：统一数据、指标、阈值选择、CI估计，避免实验间不可比\n",
        "# =====================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, matthews_corrcoef, brier_score_loss\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune\n",
        "from proteinbert.finetuning import encode_dataset, split_dataset_by_len\n",
        "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
        "\n",
        "BENCHMARK_NAME = 'anticrispr_binary'\n",
        "OUTPUT_TYPE = OutputType(False, 'binary')\n",
        "UNIQUE_LABELS = [0, 1]\n",
        "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
        "\n",
        "full_train = pd.read_csv(os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.train.csv')).dropna().drop_duplicates().reset_index(drop=True)\n",
        "full_test = pd.read_csv(os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.test.csv')).dropna().drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f'[Protocol] Train: {len(full_train)} ({(full_train.label==1).sum()}+/{(full_train.label==0).sum()}-)')\n",
        "print(f'[Protocol] Test : {len(full_test)} ({(full_test.label==1).sum()}+/{(full_test.label==0).sum()}-)')\n",
        "\n",
        "# 按计划固定 >=5 个随机种子，降低偶然性\n",
        "SEEDS = [0, 11, 22, 33, 44]\n",
        "\n",
        "\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        m = (y_prob >= bins[i]) & (y_prob < bins[i+1])\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        conf = y_prob[m].mean()\n",
        "        acc = y_true[m].mean()\n",
        "        ece += np.abs(acc - conf) * m.mean()\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def select_best_threshold(y_true, y_prob, grid=None):\n",
        "    if grid is None:\n",
        "        grid = np.linspace(0.1, 0.9, 33)\n",
        "    best_thr, best_f1 = 0.5, -1.0\n",
        "    for thr in grid:\n",
        "        y_cls = (y_prob >= thr).astype(int)\n",
        "        f1 = f1_score(y_true, y_cls, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_thr, best_f1 = float(thr), float(f1)\n",
        "    return best_thr, best_f1\n",
        "\n",
        "\n",
        "def summarize_metrics(y_true, y_prob, thr):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "    y_cls = (y_prob >= thr).astype(int)\n",
        "    return {\n",
        "        'AUC': float(roc_auc_score(y_true, y_prob)),\n",
        "        'AUPRC': float(average_precision_score(y_true, y_prob)),\n",
        "        'F1': float(f1_score(y_true, y_cls, zero_division=0)),\n",
        "        'MCC': float(matthews_corrcoef(y_true, y_cls)),\n",
        "        'Brier': float(brier_score_loss(y_true, y_prob)),\n",
        "        'ECE': float(expected_calibration_error(y_true, y_prob, n_bins=10)),\n",
        "        'Threshold': float(thr),\n",
        "    }\n",
        "\n",
        "\n",
        "def bootstrap_ci(y_true, y_prob, metric_fn, n_boot=1000, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_prob = np.asarray(y_prob)\n",
        "    n = len(y_true)\n",
        "    vals = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        yt = y_true[idx]\n",
        "        yp = y_prob[idx]\n",
        "        if len(np.unique(yt)) < 2:\n",
        "            continue\n",
        "        vals.append(metric_fn(yt, yp))\n",
        "    if len(vals) == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    return (float(np.percentile(vals, 2.5)), float(np.percentile(vals, 97.5)))\n",
        "\n",
        "\n",
        "def predict_proteinbert_probs(model_generator, input_encoder, seqs, labels, start_seq_len=512, start_batch_size=32):\n",
        "    df = pd.DataFrame({'seq': list(seqs), 'raw_y': list(labels)})\n",
        "    y_true_all, y_prob_all = [], []\n",
        "    for d, sl, bs in split_dataset_by_len(df, start_seq_len=start_seq_len, start_batch_size=start_batch_size):\n",
        "        if len(d) == 0:\n",
        "            continue\n",
        "        X, yt, sw = encode_dataset(d['seq'], d['raw_y'], input_encoder, OUTPUT_SPEC, seq_len=sl, needs_filtering=False)\n",
        "        m = (sw == 1)\n",
        "        mdl = model_generator.create_model(sl)\n",
        "        yp = mdl.predict(X, batch_size=bs).flatten()\n",
        "        y_true_all.append(yt[m].flatten())\n",
        "        y_prob_all.append(yp[m].flatten())\n",
        "    return np.concatenate(y_true_all), np.concatenate(y_prob_all)\n",
        "\n",
        "\n",
        "def run_finetune_once(train_df, valid_df, test_df, cfg):\n",
        "    pretrained_model_generator, input_encoder = load_pretrained_model()\n",
        "    mg = FinetuningModelGenerator(\n",
        "        pretrained_model_generator,\n",
        "        OUTPUT_SPEC,\n",
        "        pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs,\n",
        "        dropout_rate=cfg.get('dropout', 0.5),\n",
        "    )\n",
        "    cbs = [\n",
        "        keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-5, verbose=0),\n",
        "        keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
        "    ]\n",
        "\n",
        "    finetune(\n",
        "        mg, input_encoder, OUTPUT_SPEC,\n",
        "        train_df['seq'], train_df['label'],\n",
        "        valid_df['seq'], valid_df['label'],\n",
        "        seq_len=cfg.get('seq_len', 512),\n",
        "        batch_size=cfg.get('batch_size', 32),\n",
        "        max_epochs_per_stage=cfg.get('max_epochs', 40),\n",
        "        lr=cfg.get('lr', 1e-4),\n",
        "        begin_with_frozen_pretrained_layers=cfg.get('freeze_first', True),\n",
        "        lr_with_frozen_pretrained_layers=cfg.get('lr_frozen', 1e-2),\n",
        "        n_final_epochs=cfg.get('n_final_epochs', 1),\n",
        "        final_seq_len=cfg.get('final_seq_len', 1024),\n",
        "        final_lr=cfg.get('final_lr', 1e-5),\n",
        "        callbacks=cbs,\n",
        "    )\n",
        "\n",
        "    yv_true, yv_prob = predict_proteinbert_probs(mg, input_encoder, valid_df['seq'], valid_df['label'])\n",
        "    thr, _ = select_best_threshold(yv_true, yv_prob)\n",
        "    yt_true, yt_prob = predict_proteinbert_probs(mg, input_encoder, test_df['seq'], test_df['label'])\n",
        "    metrics = summarize_metrics(yt_true, yt_prob, thr)\n",
        "    return mg, input_encoder, metrics, (yt_true, yt_prob)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp3] 已删除旧的特征审计分支（历史实验cell）。\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验3：旧特征工程分支已移除\n",
        "# 说明：按 ProteinBERT 冲刺计划，仅保留“微调 -> 校准 -> 同构集成”主线\n",
        "# =====================================================================\n",
        "\n",
        "print('[Exp3] 已删除旧的特征审计分支（历史实验cell）。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp4] 已删除旧的特征重构/融合分支（历史实验cell）。\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验4：旧融合相关分支已移除\n",
        "# 说明：按计划不使用手工特征融合与复杂 stacking\n",
        "# =====================================================================\n",
        "\n",
        "print('[Exp4] 已删除旧的特征重构/融合分支（历史实验cell）。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026_02_12-12:28:37] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:28:37] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:28:37] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 12s 236ms/step - loss: 0.4626 - val_loss: 0.3349\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 7s 214ms/step - loss: 0.2887 - val_loss: 0.3919\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 7s 214ms/step - loss: 0.2670 - val_loss: 0.3230\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 203ms/step - loss: 0.2292 - val_loss: 0.3169\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 7s 210ms/step - loss: 0.2186 - val_loss: 0.3163\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 7s 216ms/step - loss: 0.2395 - val_loss: 0.3220\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 0.1903 - val_loss: 0.3186\n",
            "[2026_02_12-12:29:30] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:29:35] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 25s 638ms/step - loss: 0.2172 - val_loss: 0.3142\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 19s 595ms/step - loss: 0.1882 - val_loss: 0.3090\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 19s 588ms/step - loss: 0.1444 - val_loss: 0.3636\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 579ms/step - loss: 0.1200 - val_loss: 0.3378\n",
            "[2026_02_12-12:30:56] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:30:56] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:30:56] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 589ms/step - loss: 0.1816 - val_loss: 0.3306\n",
            "[Exp5][seed=0] AUC=0.8809, AUPRC=0.5894, F1=0.3958, thr=0.10\n",
            "[2026_02_12-12:31:48] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:31:48] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:31:48] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 12s 230ms/step - loss: 0.4985 - val_loss: 0.3894\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.3833 - val_loss: 0.3151\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2712 - val_loss: 0.2726\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2648 - val_loss: 0.2551\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 194ms/step - loss: 0.2350 - val_loss: 0.2710\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 193ms/step - loss: 0.1987 - val_loss: 0.2565\n",
            "[2026_02_12-12:32:31] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:32:37] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 613ms/step - loss: 0.2090 - val_loss: 0.2519\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.1730 - val_loss: 0.2504\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 570ms/step - loss: 0.1407 - val_loss: 0.2440\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.0953 - val_loss: 0.2763\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 18s 561ms/step - loss: 0.0725 - val_loss: 0.2409\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 18s 563ms/step - loss: 0.0581 - val_loss: 0.2470\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 18s 564ms/step - loss: 0.0502 - val_loss: 0.2488\n",
            "[2026_02_12-12:34:49] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:34:49] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:34:49] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 588ms/step - loss: 0.1113 - val_loss: 0.2347\n",
            "[Exp5][seed=11] AUC=0.8882, AUPRC=0.5673, F1=0.4889, thr=0.38\n",
            "[2026_02_12-12:35:41] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:35:41] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:35:41] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 12s 223ms/step - loss: 0.4705 - val_loss: 0.3112\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.3134 - val_loss: 0.2929\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2640 - val_loss: 0.2804\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2157 - val_loss: 0.2800\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2027 - val_loss: 0.2594\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 193ms/step - loss: 0.1827 - val_loss: 0.2716\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.1633 - val_loss: 0.2742\n",
            "[2026_02_12-12:36:29] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:36:34] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 592ms/step - loss: 0.1940 - val_loss: 0.2831\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 560ms/step - loss: 0.1652 - val_loss: 0.2931\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.1270 - val_loss: 0.2739\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 560ms/step - loss: 0.1278 - val_loss: 0.2781\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 18s 561ms/step - loss: 0.1085 - val_loss: 0.2825\n",
            "[2026_02_12-12:38:09] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:38:09] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:38:09] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 575ms/step - loss: 0.1583 - val_loss: 0.2794\n",
            "[Exp5][seed=22] AUC=0.8834, AUPRC=0.5756, F1=0.4286, thr=0.15\n",
            "[2026_02_12-12:39:00] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:39:00] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:39:00] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 219ms/step - loss: 0.4953 - val_loss: 0.3648\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 195ms/step - loss: 0.2862 - val_loss: 0.3735\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.2350 - val_loss: 0.3745\n",
            "[2026_02_12-12:39:24] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:39:29] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 604ms/step - loss: 0.2807 - val_loss: 0.3605\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 19s 587ms/step - loss: 0.2601 - val_loss: 0.4111\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 576ms/step - loss: 0.2405 - val_loss: 0.3628\n",
            "[2026_02_12-12:40:30] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:40:30] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:40:30] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 42s 605ms/step - loss: 0.2757 - val_loss: 0.3532\n",
            "[Exp5][seed=33] AUC=0.8678, AUPRC=0.5891, F1=0.3571, thr=0.12\n",
            "[2026_02_12-12:41:23] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:41:23] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:41:23] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 222ms/step - loss: 0.4856 - val_loss: 0.3774\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.3392 - val_loss: 0.3490\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2954 - val_loss: 0.3446\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.2292 - val_loss: 0.3346\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 0.2133 - val_loss: 0.3782\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.2254 - val_loss: 0.3371\n",
            "[2026_02_12-12:42:06] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:42:11] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 595ms/step - loss: 0.1972 - val_loss: 0.3876\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.1737 - val_loss: 0.3330\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 570ms/step - loss: 0.1437 - val_loss: 0.3369\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1046 - val_loss: 0.3407\n",
            "[2026_02_12-12:43:29] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:43:29] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:43:29] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 590ms/step - loss: 0.1748 - val_loss: 0.3276\n",
            "[Exp5][seed=44] AUC=0.8843, AUPRC=0.5877, F1=0.4706, thr=0.10\n",
            "\n",
            "[Exp5] 基线多随机种子结果:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Seed</th>\n",
              "      <th>AUC</th>\n",
              "      <th>AUPRC</th>\n",
              "      <th>F1</th>\n",
              "      <th>MCC</th>\n",
              "      <th>Brier</th>\n",
              "      <th>ECE</th>\n",
              "      <th>Threshold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.880917</td>\n",
              "      <td>0.589370</td>\n",
              "      <td>0.395833</td>\n",
              "      <td>0.357469</td>\n",
              "      <td>0.056421</td>\n",
              "      <td>0.031150</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>0.888166</td>\n",
              "      <td>0.567347</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.452864</td>\n",
              "      <td>0.058994</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>0.375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>0.883432</td>\n",
              "      <td>0.575615</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.370810</td>\n",
              "      <td>0.056735</td>\n",
              "      <td>0.029448</td>\n",
              "      <td>0.150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>0.867751</td>\n",
              "      <td>0.589122</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>0.323103</td>\n",
              "      <td>0.058063</td>\n",
              "      <td>0.042527</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>44</td>\n",
              "      <td>0.884320</td>\n",
              "      <td>0.587653</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.439933</td>\n",
              "      <td>0.055433</td>\n",
              "      <td>0.017079</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Seed       AUC     AUPRC        F1       MCC     Brier       ECE  Threshold\n",
              "0     0  0.880917  0.589370  0.395833  0.357469  0.056421  0.031150      0.100\n",
              "1    11  0.888166  0.567347  0.488889  0.452864  0.058994  0.036331      0.375\n",
              "2    22  0.883432  0.575615  0.428571  0.370810  0.056735  0.029448      0.150\n",
              "3    33  0.867751  0.589122  0.357143  0.323103  0.058063  0.042527      0.125\n",
              "4    44  0.884320  0.587653  0.470588  0.439933  0.055433  0.017079      0.100"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp5] 均值±标准差:\n",
            "  AUC: 0.8809 ± 0.0078\n",
            "  AUPRC: 0.5818 ± 0.0099\n",
            "  F1: 0.4282 ± 0.0538\n",
            "  MCC: 0.3888 ± 0.0555\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验5：ProteinBERT原始微调流程严格复现（主锚点）\n",
        "# 目标：建立可信基线，后续所有优化必须与其比较\n",
        "# =====================================================================\n",
        "\n",
        "baseline_cfg = dict(\n",
        "    name='baseline_cell1',\n",
        "    dropout=0.5,\n",
        "    seq_len=512,\n",
        "    batch_size=32,\n",
        "    max_epochs=40,\n",
        "    lr=1e-4,\n",
        "    freeze_first=True,\n",
        "    lr_frozen=1e-2,\n",
        "    n_final_epochs=1,\n",
        "    final_seq_len=1024,\n",
        "    final_lr=1e-5,\n",
        ")\n",
        "\n",
        "baseline_rows = []\n",
        "baseline_probs = []\n",
        "for seed in SEEDS:\n",
        "    tr_df, va_df = train_test_split(\n",
        "        full_train, test_size=0.1, stratify=full_train['label'], random_state=seed\n",
        "    )\n",
        "    _, _, met, (yt, yp) = run_finetune_once(tr_df.reset_index(drop=True), va_df.reset_index(drop=True), full_test, baseline_cfg)\n",
        "    met['Seed'] = seed\n",
        "    baseline_rows.append(met)\n",
        "    baseline_probs.append((yt, yp))\n",
        "    print(f\"[Exp5][seed={seed}] AUC={met['AUC']:.4f}, AUPRC={met['AUPRC']:.4f}, F1={met['F1']:.4f}, thr={met['Threshold']:.2f}\")\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_rows)\n",
        "print('\\n[Exp5] 基线多随机种子结果:')\n",
        "display(baseline_df[['Seed','AUC','AUPRC','F1','MCC','Brier','ECE','Threshold']])\n",
        "\n",
        "print('[Exp5] 均值±标准差:')\n",
        "for k in ['AUC','AUPRC','F1','MCC']:\n",
        "    print(f'  {k}: {baseline_df[k].mean():.4f} ± {baseline_df[k].std(ddof=1):.4f}')\n",
        "\n",
        "BASELINE_RESULT = baseline_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026_02_12-12:44:22] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:44:22] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:44:22] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 223ms/step - loss: 0.4542 - val_loss: 0.3619\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.3182 - val_loss: 0.3673\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 0.2605 - val_loss: 0.3381\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.2075 - val_loss: 0.3394\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.2236 - val_loss: 0.3321\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.2386 - val_loss: 0.3354\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 193ms/step - loss: 0.2166 - val_loss: 0.3329\n",
            "[2026_02_12-12:45:11] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:45:16] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 598ms/step - loss: 0.2129 - val_loss: 0.3314\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 564ms/step - loss: 0.1833 - val_loss: 0.3378\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 567ms/step - loss: 0.1543 - val_loss: 0.3368\n",
            "[2026_02_12-12:46:15] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:46:15] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:46:15] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 594ms/step - loss: 0.2120 - val_loss: 0.3381\n",
            "[Exp6][G1_baseline_like][seed=0] AUC=0.8870, AUPRC=0.5926, F1=0.4595\n",
            "[2026_02_12-12:47:08] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:47:08] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:47:08] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 221ms/step - loss: 0.5073 - val_loss: 0.2848\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.3266 - val_loss: 0.2695\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.2549 - val_loss: 0.2629\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.2340 - val_loss: 0.2661\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2335 - val_loss: 0.2547\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.2093 - val_loss: 0.2450\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 195ms/step - loss: 0.2026 - val_loss: 0.2435\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.1972 - val_loss: 0.2488\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.1839 - val_loss: 0.2449\n",
            "[2026_02_12-12:48:09] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:48:14] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 598ms/step - loss: 0.2037 - val_loss: 0.2362\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1771 - val_loss: 0.2382\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.1432 - val_loss: 0.2381\n",
            "[2026_02_12-12:49:13] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:49:13] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:49:13] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 595ms/step - loss: 0.2042 - val_loss: 0.2421\n",
            "[Exp6][G1_baseline_like][seed=11] AUC=0.9084, AUPRC=0.6581, F1=0.5373\n",
            "[2026_02_12-12:50:06] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:50:06] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:50:06] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 210ms/step - loss: 0.5172 - val_loss: 0.3592\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.3011 - val_loss: 0.3635\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2849 - val_loss: 0.2915\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2424 - val_loss: 0.2839\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2393 - val_loss: 0.2887\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2175 - val_loss: 0.2811\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2309 - val_loss: 0.2831\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2034 - val_loss: 0.2827\n",
            "[2026_02_12-12:50:59] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:51:05] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 603ms/step - loss: 0.2332 - val_loss: 0.3238\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 549ms/step - loss: 0.1925 - val_loss: 0.2860\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1481 - val_loss: 0.2957\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 571ms/step - loss: 0.1071 - val_loss: 0.3115\n",
            "[2026_02_12-12:52:21] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:52:21] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:52:21] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 588ms/step - loss: 0.1980 - val_loss: 0.3049\n",
            "[Exp6][G1_baseline_like][seed=22] AUC=0.8822, AUPRC=0.5937, F1=0.4634\n",
            "[2026_02_12-12:53:14] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:53:14] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:53:14] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 217ms/step - loss: 0.4907 - val_loss: 0.3930\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2994 - val_loss: 0.3840\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2325 - val_loss: 0.3857\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2214 - val_loss: 0.3800\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2058 - val_loss: 0.3830\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2126 - val_loss: 0.3828\n",
            "[2026_02_12-12:53:55] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:54:01] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 583ms/step - loss: 0.2050 - val_loss: 0.3765\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 543ms/step - loss: 0.1705 - val_loss: 0.4063\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1404 - val_loss: 0.4196\n",
            "[2026_02_12-12:54:58] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:54:58] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:54:59] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 567ms/step - loss: 0.2036 - val_loss: 0.3696\n",
            "[Exp6][G1_baseline_like][seed=33] AUC=0.8848, AUPRC=0.6026, F1=0.3960\n",
            "[2026_02_12-12:55:50] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:55:50] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:55:50] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 215ms/step - loss: 0.4743 - val_loss: 0.3488\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 172ms/step - loss: 0.3066 - val_loss: 0.3382\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.2352 - val_loss: 0.3205\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2148 - val_loss: 0.3441\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2090 - val_loss: 0.3356\n",
            "[2026_02_12-12:56:24] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:56:30] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 588ms/step - loss: 0.2192 - val_loss: 0.3154\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1810 - val_loss: 0.3242\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.1578 - val_loss: 0.3258\n",
            "[2026_02_12-12:57:28] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-12:57:28] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-12:57:28] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 572ms/step - loss: 0.2230 - val_loss: 0.3059\n",
            "[Exp6][G1_baseline_like][seed=44] AUC=0.8967, AUPRC=0.5826, F1=0.4400\n",
            "[2026_02_12-12:58:19] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:58:19] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-12:58:20] Training with frozen pretrained layers...\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 10s 205ms/step - loss: 0.4450 - val_loss: 0.3432\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2854 - val_loss: 0.3195\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2677 - val_loss: 0.3319\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2054 - val_loss: 0.3204\n",
            "[2026_02_12-12:58:48] Training the entire fine-tuned model...\n",
            "[2026_02_12-12:58:53] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 23s 579ms/step - loss: 0.2389 - val_loss: 0.3280\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.2066 - val_loss: 0.3297\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.1575 - val_loss: 0.3136\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 17s 536ms/step - loss: 0.1468 - val_loss: 0.3368\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1386 - val_loss: 0.3275\n",
            "[2026_02_12-13:00:26] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:00:26] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:00:26] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 580ms/step - loss: 0.1865 - val_loss: 0.3347\n",
            "[Exp6][G2_shorter_train][seed=0] AUC=0.8936, AUPRC=0.6211, F1=0.4301\n",
            "[2026_02_12-13:01:18] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:01:18] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:01:18] Training with frozen pretrained layers...\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 11s 212ms/step - loss: 0.4383 - val_loss: 0.2880\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2974 - val_loss: 0.2976\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2346 - val_loss: 0.2668\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2239 - val_loss: 0.2689\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2121 - val_loss: 0.2667\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2646 - val_loss: 0.2663\n",
            "Epoch 7/30\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2424 - val_loss: 0.2643\n",
            "Epoch 8/30\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2126 - val_loss: 0.2630\n",
            "Epoch 9/30\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2244 - val_loss: 0.2632\n",
            "Epoch 10/30\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2074 - val_loss: 0.2630\n",
            "[2026_02_12-13:02:22] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:02:27] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 22s 587ms/step - loss: 0.2204 - val_loss: 0.2509\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 17s 536ms/step - loss: 0.1790 - val_loss: 0.2497\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 18s 556ms/step - loss: 0.1258 - val_loss: 0.2595\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.0852 - val_loss: 0.2532\n",
            "[2026_02_12-13:03:42] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:03:42] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:03:42] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 567ms/step - loss: 0.1757 - val_loss: 0.2445\n",
            "[Exp6][G2_shorter_train][seed=11] AUC=0.8914, AUPRC=0.6028, F1=0.5660\n",
            "[2026_02_12-13:04:34] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:04:34] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:04:34] Training with frozen pretrained layers...\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 10s 210ms/step - loss: 0.4704 - val_loss: 0.3098\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2835 - val_loss: 0.2809\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2497 - val_loss: 0.2796\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1997 - val_loss: 0.2551\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.1929 - val_loss: 0.2608\n",
            "Epoch 6/30\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1809 - val_loss: 0.2657\n",
            "[2026_02_12-13:05:14] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:05:19] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 23s 601ms/step - loss: 0.2036 - val_loss: 0.2834\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 18s 549ms/step - loss: 0.1638 - val_loss: 0.2585\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.1415 - val_loss: 0.2855\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 18s 576ms/step - loss: 0.1136 - val_loss: 0.2783\n",
            "[2026_02_12-13:06:37] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:06:37] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:06:37] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 578ms/step - loss: 0.1750 - val_loss: 0.2646\n",
            "[Exp6][G2_shorter_train][seed=22] AUC=0.8899, AUPRC=0.5874, F1=0.4615\n",
            "[2026_02_12-13:07:29] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:07:29] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:07:29] Training with frozen pretrained layers...\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 10s 209ms/step - loss: 0.4870 - val_loss: 0.3941\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.2978 - val_loss: 0.4134\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2459 - val_loss: 0.3676\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.2221 - val_loss: 0.3715\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.2116 - val_loss: 0.3713\n",
            "[2026_02_12-13:08:03] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:08:09] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 24s 600ms/step - loss: 0.2356 - val_loss: 0.3546\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 18s 571ms/step - loss: 0.1963 - val_loss: 0.3782\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.1628 - val_loss: 0.3725\n",
            "[2026_02_12-13:09:09] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:09:09] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:09:09] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 42s 597ms/step - loss: 0.2243 - val_loss: 0.3512\n",
            "[Exp6][G2_shorter_train][seed=33] AUC=0.8590, AUPRC=0.5298, F1=0.4375\n",
            "[2026_02_12-13:10:02] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:10:02] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:10:02] Training with frozen pretrained layers...\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 10s 222ms/step - loss: 0.4397 - val_loss: 0.3569\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2799 - val_loss: 0.3460\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.2486 - val_loss: 0.3434\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.2242 - val_loss: 0.3616\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 6s 192ms/step - loss: 0.1835 - val_loss: 0.3528\n",
            "[2026_02_12-13:10:38] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:10:43] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/30\n",
            "32/32 [==============================] - 23s 595ms/step - loss: 0.2285 - val_loss: 0.3536\n",
            "Epoch 2/30\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.1792 - val_loss: 0.3368\n",
            "Epoch 3/30\n",
            "32/32 [==============================] - 18s 558ms/step - loss: 0.1621 - val_loss: 0.3301\n",
            "Epoch 4/30\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1200 - val_loss: 0.3509\n",
            "Epoch 5/30\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.0801 - val_loss: 0.3660\n",
            "[2026_02_12-13:12:18] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:12:18] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:12:18] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 585ms/step - loss: 0.1632 - val_loss: 0.3254\n",
            "[Exp6][G2_shorter_train][seed=44] AUC=0.8802, AUPRC=0.5992, F1=0.4667\n",
            "[2026_02_12-13:13:11] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:13:11] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:13:11] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 221ms/step - loss: 0.4742 - val_loss: 0.3628\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2961 - val_loss: 0.3283\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2550 - val_loss: 0.3505\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.2211 - val_loss: 0.3241\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2154 - val_loss: 0.3213\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2230 - val_loss: 0.3238\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.1861 - val_loss: 0.3229\n",
            "[2026_02_12-13:13:58] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:14:04] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 586ms/step - loss: 0.2095 - val_loss: 0.3128\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.1783 - val_loss: 0.3229\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1404 - val_loss: 0.3169\n",
            "[Exp6][G3_no_final_stage][seed=0] AUC=0.8896, AUPRC=0.6093, F1=0.4211\n",
            "[2026_02_12-13:15:08] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:15:08] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:15:08] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 207ms/step - loss: 0.4892 - val_loss: 0.2959\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2912 - val_loss: 0.2759\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2536 - val_loss: 0.3055\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2467 - val_loss: 0.2718\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2079 - val_loss: 0.2663\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2109 - val_loss: 0.2639\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2151 - val_loss: 0.2614\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1946 - val_loss: 0.2606\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2001 - val_loss: 0.2581\n",
            "Epoch 10/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.1919 - val_loss: 0.2680\n",
            "Epoch 11/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.1908 - val_loss: 0.2529\n",
            "Epoch 12/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.1684 - val_loss: 0.2514\n",
            "Epoch 13/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1895 - val_loss: 0.2535\n",
            "Epoch 14/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1767 - val_loss: 0.2535\n",
            "[2026_02_12-13:16:35] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:16:40] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 616ms/step - loss: 0.1931 - val_loss: 0.2472\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.1578 - val_loss: 0.2763\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1437 - val_loss: 0.2489\n",
            "[Exp6][G3_no_final_stage][seed=11] AUC=0.9139, AUPRC=0.6876, F1=0.4792\n",
            "[2026_02_12-13:17:45] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:17:45] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:17:45] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 215ms/step - loss: 0.4665 - val_loss: 0.2902\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.3226 - val_loss: 0.3017\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2431 - val_loss: 0.2823\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2337 - val_loss: 0.2783\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 193ms/step - loss: 0.2373 - val_loss: 0.2877\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2119 - val_loss: 0.2779\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2154 - val_loss: 0.2825\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 197ms/step - loss: 0.2086 - val_loss: 0.2840\n",
            "[2026_02_12-13:18:39] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:18:44] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 569ms/step - loss: 0.2169 - val_loss: 0.2671\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 566ms/step - loss: 0.1868 - val_loss: 0.2992\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 567ms/step - loss: 0.1528 - val_loss: 0.3002\n",
            "[Exp6][G3_no_final_stage][seed=22] AUC=0.8874, AUPRC=0.6032, F1=0.4667\n",
            "[2026_02_12-13:19:49] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:19:49] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:19:49] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 232ms/step - loss: 0.4930 - val_loss: 0.3975\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.3189 - val_loss: 0.3892\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2296 - val_loss: 0.3816\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 5s 170ms/step - loss: 0.2419 - val_loss: 0.3928\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.1998 - val_loss: 0.3886\n",
            "[2026_02_12-13:20:23] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:20:28] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 596ms/step - loss: 0.2064 - val_loss: 0.3734\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1838 - val_loss: 0.3771\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1569 - val_loss: 0.3811\n",
            "[Exp6][G3_no_final_stage][seed=33] AUC=0.8723, AUPRC=0.5797, F1=0.3860\n",
            "[2026_02_12-13:21:33] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:21:34] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:21:34] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 207ms/step - loss: 0.4487 - val_loss: 0.3672\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.3089 - val_loss: 0.3367\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2479 - val_loss: 0.3499\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2388 - val_loss: 0.3287\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2337 - val_loss: 0.3204\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 5s 171ms/step - loss: 0.2173 - val_loss: 0.3243\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.2009 - val_loss: 0.3245\n",
            "[2026_02_12-13:22:19] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:22:23] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 583ms/step - loss: 0.2087 - val_loss: 0.3230\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 555ms/step - loss: 0.1772 - val_loss: 0.3310\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 539ms/step - loss: 0.1418 - val_loss: 0.3343\n",
            "[Exp6][G3_no_final_stage][seed=44] AUC=0.8883, AUPRC=0.6003, F1=0.4571\n",
            "[2026_02_12-13:23:26] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:23:26] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:23:26] Training with frozen pretrained layers...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 10s 203ms/step - loss: 0.4789 - val_loss: 0.3381\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.3225 - val_loss: 0.3259\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.2656 - val_loss: 0.3272\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2406 - val_loss: 0.3195\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2096 - val_loss: 0.3278\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 5s 172ms/step - loss: 0.2116 - val_loss: 0.3157\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.1928 - val_loss: 0.3166\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2064 - val_loss: 0.3166\n",
            "[2026_02_12-13:24:17] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:24:22] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 22s 581ms/step - loss: 0.2062 - val_loss: 0.3184\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 547ms/step - loss: 0.1807 - val_loss: 0.3211\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.1734 - val_loss: 0.3275\n",
            "[2026_02_12-13:25:20] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:25:20] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:25:20] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 561ms/step - loss: 0.2118 - val_loss: 0.3332\n",
            "[Exp6][G4_lower_unfrozen_lr][seed=0] AUC=0.8828, AUPRC=0.6143, F1=0.4828\n",
            "[2026_02_12-13:26:10] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:26:10] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:26:10] Training with frozen pretrained layers...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 10s 199ms/step - loss: 0.4698 - val_loss: 0.3034\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2953 - val_loss: 0.2705\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2638 - val_loss: 0.2498\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2267 - val_loss: 0.2504\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2399 - val_loss: 0.2519\n",
            "[2026_02_12-13:26:44] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:26:50] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 22s 575ms/step - loss: 0.2316 - val_loss: 0.2473\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 17s 526ms/step - loss: 0.2109 - val_loss: 0.2374\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 17s 528ms/step - loss: 0.1927 - val_loss: 0.2403\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 17s 545ms/step - loss: 0.1779 - val_loss: 0.2350\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1770 - val_loss: 0.2328\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 17s 537ms/step - loss: 0.1683 - val_loss: 0.2348\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.1779 - val_loss: 0.2369\n",
            "[2026_02_12-13:28:56] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:28:56] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:28:56] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 575ms/step - loss: 0.2158 - val_loss: 0.2308\n",
            "[Exp6][G4_lower_unfrozen_lr][seed=11] AUC=0.8936, AUPRC=0.6008, F1=0.4478\n",
            "[2026_02_12-13:29:47] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:29:47] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:29:47] Training with frozen pretrained layers...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 11s 217ms/step - loss: 0.4851 - val_loss: 0.3155\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.3438 - val_loss: 0.2835\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2477 - val_loss: 0.2796\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2317 - val_loss: 0.2686\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2007 - val_loss: 0.3262\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1847 - val_loss: 0.2605\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1954 - val_loss: 0.2683\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 5s 169ms/step - loss: 0.1877 - val_loss: 0.2726\n",
            "[2026_02_12-13:30:39] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:30:44] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 23s 590ms/step - loss: 0.1801 - val_loss: 0.2706\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 566ms/step - loss: 0.1673 - val_loss: 0.2689\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 18s 563ms/step - loss: 0.1551 - val_loss: 0.2768\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 563ms/step - loss: 0.1377 - val_loss: 0.2777\n",
            "[2026_02_12-13:32:01] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:32:01] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:32:01] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 591ms/step - loss: 0.1867 - val_loss: 0.2711\n",
            "[Exp6][G4_lower_unfrozen_lr][seed=22] AUC=0.8982, AUPRC=0.6337, F1=0.5246\n",
            "[2026_02_12-13:32:54] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:32:54] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:32:54] Training with frozen pretrained layers...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 10s 207ms/step - loss: 0.4629 - val_loss: 0.3854\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.3230 - val_loss: 0.3636\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2664 - val_loss: 0.3758\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2080 - val_loss: 0.3713\n",
            "[2026_02_12-13:33:23] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:33:29] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 24s 601ms/step - loss: 0.2380 - val_loss: 0.3640\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 564ms/step - loss: 0.2183 - val_loss: 0.3657\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.2069 - val_loss: 0.3630\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 570ms/step - loss: 0.2003 - val_loss: 0.3647\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 569ms/step - loss: 0.1999 - val_loss: 0.3668\n",
            "[2026_02_12-13:35:05] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:35:05] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:35:05] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 42s 598ms/step - loss: 0.2239 - val_loss: 0.3550\n",
            "[Exp6][G4_lower_unfrozen_lr][seed=33] AUC=0.8746, AUPRC=0.5882, F1=0.3761\n",
            "[2026_02_12-13:35:58] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:35:58] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:35:58] Training with frozen pretrained layers...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 12s 220ms/step - loss: 0.4870 - val_loss: 0.3553\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2850 - val_loss: 0.3318\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2418 - val_loss: 0.3461\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2158 - val_loss: 0.3483\n",
            "[2026_02_12-13:36:28] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:36:33] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 23s 590ms/step - loss: 0.2399 - val_loss: 0.3222\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.2267 - val_loss: 0.3270\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.2167 - val_loss: 0.3191\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 561ms/step - loss: 0.2091 - val_loss: 0.3196\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 563ms/step - loss: 0.2011 - val_loss: 0.3194\n",
            "[2026_02_12-13:38:08] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:38:08] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:38:08] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 587ms/step - loss: 0.2316 - val_loss: 0.3101\n",
            "[Exp6][G4_lower_unfrozen_lr][seed=44] AUC=0.8766, AUPRC=0.5623, F1=0.4923\n",
            "[2026_02_12-13:39:01] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:39:01] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:39:01] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 12s 252ms/step - loss: 0.4697 - val_loss: 0.3602\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2559 - val_loss: 0.3288\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2196 - val_loss: 0.3362\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.1914 - val_loss: 0.3281\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.1841 - val_loss: 0.3324\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.1808 - val_loss: 0.3243\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.1854 - val_loss: 0.3247\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.1845 - val_loss: 0.3248\n",
            "[2026_02_12-13:39:55] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:40:00] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 593ms/step - loss: 0.1887 - val_loss: 0.3366\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1504 - val_loss: 0.3273\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.1187 - val_loss: 0.3523\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.0792 - val_loss: 0.3539\n",
            "[2026_02_12-13:41:17] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:41:17] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:41:17] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 42s 599ms/step - loss: 0.1531 - val_loss: 0.3303\n",
            "[Exp6][G5_less_dropout][seed=0] AUC=0.8874, AUPRC=0.6040, F1=0.4130\n",
            "[2026_02_12-13:42:10] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:42:10] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:42:10] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 12s 231ms/step - loss: 0.4725 - val_loss: 0.2894\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2790 - val_loss: 0.2609\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2501 - val_loss: 0.2720\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2130 - val_loss: 0.2463\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.1948 - val_loss: 0.2446\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2004 - val_loss: 0.2450\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.1852 - val_loss: 0.2481\n",
            "[2026_02_12-13:42:59] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:43:03] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 595ms/step - loss: 0.1903 - val_loss: 0.2366\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 567ms/step - loss: 0.1317 - val_loss: 0.2369\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 564ms/step - loss: 0.0886 - val_loss: 0.2313\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 564ms/step - loss: 0.0699 - val_loss: 0.2393\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.0562 - val_loss: 0.2398\n",
            "[2026_02_12-13:44:39] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:44:39] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:44:39] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 589ms/step - loss: 0.1051 - val_loss: 0.2245\n",
            "[Exp6][G5_less_dropout][seed=11] AUC=0.9064, AUPRC=0.6116, F1=0.5000\n",
            "[2026_02_12-13:45:32] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:45:33] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:45:33] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 244ms/step - loss: 0.4765 - val_loss: 0.2889\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2842 - val_loss: 0.2912\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2309 - val_loss: 0.2782\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2207 - val_loss: 0.2728\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2062 - val_loss: 0.2771\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.1855 - val_loss: 0.2700\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.2059 - val_loss: 0.2708\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2006 - val_loss: 0.2706\n",
            "[2026_02_12-13:46:25] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:46:30] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 608ms/step - loss: 0.2040 - val_loss: 0.2736\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 560ms/step - loss: 0.1705 - val_loss: 0.2695\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 569ms/step - loss: 0.1240 - val_loss: 0.3555\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 572ms/step - loss: 0.0874 - val_loss: 0.3353\n",
            "[2026_02_12-13:47:48] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:47:48] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:47:48] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 42s 600ms/step - loss: 0.1581 - val_loss: 0.2814\n",
            "[Exp6][G5_less_dropout][seed=22] AUC=0.8765, AUPRC=0.5837, F1=0.5333\n",
            "[2026_02_12-13:48:41] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:48:42] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:48:42] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 221ms/step - loss: 0.4748 - val_loss: 0.3884\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2650 - val_loss: 0.3856\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2407 - val_loss: 0.3934\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2076 - val_loss: 0.3812\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.1937 - val_loss: 0.3810\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.1836 - val_loss: 0.3812\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1564 - val_loss: 0.3797\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.1611 - val_loss: 0.3793\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1670 - val_loss: 0.3820\n",
            "Epoch 10/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2000 - val_loss: 0.3813\n",
            "[2026_02_12-13:49:47] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:49:52] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 584ms/step - loss: 0.1735 - val_loss: 0.4215\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 543ms/step - loss: 0.1308 - val_loss: 0.4384\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.0835 - val_loss: 0.4456\n",
            "[2026_02_12-13:50:50] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:50:50] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:50:50] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 587ms/step - loss: 0.1596 - val_loss: 0.3910\n",
            "[Exp6][G5_less_dropout][seed=33] AUC=0.8867, AUPRC=0.6082, F1=0.4186\n",
            "[2026_02_12-13:51:43] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:51:43] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:51:43] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 240ms/step - loss: 0.4752 - val_loss: 0.3551\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2530 - val_loss: 0.3501\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2304 - val_loss: 0.3243\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.1942 - val_loss: 0.3295\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1752 - val_loss: 0.3213\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.1854 - val_loss: 0.3230\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.1721 - val_loss: 0.3244\n",
            "[2026_02_12-13:52:31] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:52:36] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 598ms/step - loss: 0.1743 - val_loss: 0.3472\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 567ms/step - loss: 0.1331 - val_loss: 0.3218\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 563ms/step - loss: 0.1079 - val_loss: 0.3534\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 560ms/step - loss: 0.0738 - val_loss: 0.3475\n",
            "[2026_02_12-13:53:53] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-13:53:53] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-13:53:53] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 589ms/step - loss: 0.1469 - val_loss: 0.3190\n",
            "[Exp6][G5_less_dropout][seed=44] AUC=0.8994, AUPRC=0.6076, F1=0.4471\n",
            "[2026_02_12-13:54:46] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:54:46] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:54:46] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 246ms/step - loss: 0.4452 - val_loss: 0.3678\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2961 - val_loss: 0.3303\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2316 - val_loss: 0.3308\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2141 - val_loss: 0.3389\n",
            "[2026_02_12-13:55:16] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:55:21] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 592ms/step - loss: 0.2474 - val_loss: 0.3358\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.2107 - val_loss: 0.3208\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 549ms/step - loss: 0.1718 - val_loss: 0.3256\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1419 - val_loss: 0.3264\n",
            "[2026_02_12-13:56:37] Training on final epochs of sequence length 512...\n",
            "[2026_02_12-13:56:37] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:56:37] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "32/32 [==============================] - 23s 594ms/step - loss: 0.1717 - val_loss: 0.3237\n",
            "[Exp6][G6_final_len512][seed=0] AUC=0.8732, AUPRC=0.5644, F1=0.4211\n",
            "[2026_02_12-13:57:12] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:57:12] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:57:12] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 240ms/step - loss: 0.4887 - val_loss: 0.3097\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.3046 - val_loss: 0.2901\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2429 - val_loss: 0.2566\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2190 - val_loss: 0.2526\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 0.2124 - val_loss: 0.2431\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.2055 - val_loss: 0.2715\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 199ms/step - loss: 0.2071 - val_loss: 0.2518\n",
            "[2026_02_12-13:58:01] Training the entire fine-tuned model...\n",
            "[2026_02_12-13:58:06] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 599ms/step - loss: 0.1812 - val_loss: 0.2360\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1507 - val_loss: 0.2353\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 550ms/step - loss: 0.1256 - val_loss: 0.2300\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.1017 - val_loss: 0.2294\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.0672 - val_loss: 0.2920\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.0527 - val_loss: 0.3007\n",
            "[2026_02_12-13:59:58] Training on final epochs of sequence length 512...\n",
            "[2026_02_12-13:59:58] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-13:59:58] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "32/32 [==============================] - 22s 581ms/step - loss: 0.0739 - val_loss: 0.2317\n",
            "[Exp6][G6_final_len512][seed=11] AUC=0.9114, AUPRC=0.6243, F1=0.5231\n",
            "[2026_02_12-14:00:32] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:00:32] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:00:32] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 218ms/step - loss: 0.4788 - val_loss: 0.3345\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.3228 - val_loss: 0.3030\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2672 - val_loss: 0.2792\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2708 - val_loss: 0.3666\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2230 - val_loss: 0.2786\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2160 - val_loss: 0.2862\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2062 - val_loss: 0.2766\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.1728 - val_loss: 0.2732\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.1926 - val_loss: 0.2754\n",
            "Epoch 10/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.1723 - val_loss: 0.2748\n",
            "[2026_02_12-14:01:37] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:01:42] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 24s 602ms/step - loss: 0.1971 - val_loss: 0.2647\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.1585 - val_loss: 0.2896\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.1157 - val_loss: 0.2980\n",
            "[2026_02_12-14:02:42] Training on final epochs of sequence length 512...\n",
            "[2026_02_12-14:02:42] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:02:42] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "32/32 [==============================] - 23s 597ms/step - loss: 0.1634 - val_loss: 0.2764\n",
            "[Exp6][G6_final_len512][seed=22] AUC=0.9070, AUPRC=0.6425, F1=0.5294\n",
            "[2026_02_12-14:03:16] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:03:16] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:03:16] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.4632 - val_loss: 0.3799\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2944 - val_loss: 0.4219\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2542 - val_loss: 0.3815\n",
            "[2026_02_12-14:03:39] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:03:45] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 590ms/step - loss: 0.2891 - val_loss: 0.3608\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 558ms/step - loss: 0.2614 - val_loss: 0.3680\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 558ms/step - loss: 0.2272 - val_loss: 0.3710\n",
            "[2026_02_12-14:04:44] Training on final epochs of sequence length 512...\n",
            "[2026_02_12-14:04:44] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:04:44] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "32/32 [==============================] - 23s 590ms/step - loss: 0.2582 - val_loss: 0.3609\n",
            "[Exp6][G6_final_len512][seed=33] AUC=0.8669, AUPRC=0.5481, F1=0.3846\n",
            "[2026_02_12-14:05:17] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:05:17] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:05:17] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.4640 - val_loss: 0.4792\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.3653 - val_loss: 0.3360\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2375 - val_loss: 0.3557\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.2115 - val_loss: 0.3443\n",
            "[2026_02_12-14:05:48] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:05:53] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 581ms/step - loss: 0.2347 - val_loss: 0.3332\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 558ms/step - loss: 0.2076 - val_loss: 0.3369\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.1546 - val_loss: 0.3329\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.1374 - val_loss: 0.3380\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 543ms/step - loss: 0.1308 - val_loss: 0.3387\n",
            "[2026_02_12-14:07:26] Training on final epochs of sequence length 512...\n",
            "[2026_02_12-14:07:26] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:07:26] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "32/32 [==============================] - 23s 602ms/step - loss: 0.1424 - val_loss: 0.3372\n",
            "[Exp6][G6_final_len512][seed=44] AUC=0.8612, AUPRC=0.5526, F1=0.4474\n",
            "[2026_02_12-14:08:01] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:08:01] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:08:01] Training the entire fine-tuned model...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 25s 580ms/step - loss: 0.6454 - val_loss: 0.5351\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.5176 - val_loss: 0.4785\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.4830 - val_loss: 0.4277\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 546ms/step - loss: 0.4112 - val_loss: 0.3862\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.3354 - val_loss: 0.3596\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.2841 - val_loss: 0.3576\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.2463 - val_loss: 0.3419\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.2246 - val_loss: 0.3379\n",
            "Epoch 9/35\n",
            "32/32 [==============================] - 17s 539ms/step - loss: 0.1754 - val_loss: 0.4785\n",
            "Epoch 10/35\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.1375 - val_loss: 0.4350\n",
            "[2026_02_12-14:11:05] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:11:05] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:11:05] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 580ms/step - loss: 0.2309 - val_loss: 0.3604\n",
            "[Exp6][G7_no_freeze][seed=0] AUC=0.7834, AUPRC=0.5120, F1=0.4000\n",
            "[2026_02_12-14:11:56] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:11:56] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:11:56] Training the entire fine-tuned model...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 25s 591ms/step - loss: 0.6446 - val_loss: 0.5158\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.4919 - val_loss: 0.4586\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.4578 - val_loss: 0.4031\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 568ms/step - loss: 0.4024 - val_loss: 0.3489\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 565ms/step - loss: 0.3647 - val_loss: 0.3325\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.3043 - val_loss: 0.2782\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.2714 - val_loss: 0.2824\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 18s 554ms/step - loss: 0.2107 - val_loss: 0.2773\n",
            "Epoch 9/35\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.2043 - val_loss: 0.2757\n",
            "Epoch 10/35\n",
            "32/32 [==============================] - 18s 569ms/step - loss: 0.1686 - val_loss: 0.2756\n",
            "Epoch 11/35\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.1506 - val_loss: 0.2748\n",
            "Epoch 12/35\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.1477 - val_loss: 0.2729\n",
            "Epoch 13/35\n",
            "32/32 [==============================] - 18s 562ms/step - loss: 0.1660 - val_loss: 0.2907\n",
            "Epoch 14/35\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.1503 - val_loss: 0.2985\n",
            "[2026_02_12-14:16:15] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:16:15] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:16:15] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 575ms/step - loss: 0.2009 - val_loss: 0.2797\n",
            "[Exp6][G7_no_freeze][seed=11] AUC=0.8824, AUPRC=0.5389, F1=0.5091\n",
            "[2026_02_12-14:17:06] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:17:06] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:17:06] Training the entire fine-tuned model...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 24s 584ms/step - loss: 0.6206 - val_loss: 0.5197\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.5019 - val_loss: 0.4729\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.4421 - val_loss: 0.4005\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 18s 558ms/step - loss: 0.3841 - val_loss: 0.3843\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.3504 - val_loss: 0.3677\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.3095 - val_loss: 0.3276\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.2662 - val_loss: 0.3316\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.2199 - val_loss: 0.3344\n",
            "[2026_02_12-14:19:35] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:19:36] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:19:36] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 572ms/step - loss: 0.3022 - val_loss: 0.3242\n",
            "[Exp6][G7_no_freeze][seed=22] AUC=0.8226, AUPRC=0.3998, F1=0.4000\n",
            "[2026_02_12-14:20:27] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:20:27] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:20:27] Training the entire fine-tuned model...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 25s 610ms/step - loss: 0.6376 - val_loss: 0.5211\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 555ms/step - loss: 0.4766 - val_loss: 0.4648\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 17s 540ms/step - loss: 0.4318 - val_loss: 0.4234\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 17s 540ms/step - loss: 0.4044 - val_loss: 0.4350\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.3136 - val_loss: 0.3764\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 17s 539ms/step - loss: 0.3289 - val_loss: 0.3818\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 17s 545ms/step - loss: 0.3119 - val_loss: 0.3962\n",
            "[2026_02_12-14:22:38] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:22:38] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:22:38] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 569ms/step - loss: 0.3435 - val_loss: 0.3919\n",
            "[Exp6][G7_no_freeze][seed=33] AUC=0.7862, AUPRC=0.4189, F1=0.3696\n",
            "[2026_02_12-14:23:29] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:23:29] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:23:29] Training the entire fine-tuned model...\n",
            "Epoch 1/35\n",
            "32/32 [==============================] - 25s 575ms/step - loss: 0.6298 - val_loss: 0.5163\n",
            "Epoch 2/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.4817 - val_loss: 0.4674\n",
            "Epoch 3/35\n",
            "32/32 [==============================] - 17s 540ms/step - loss: 0.4490 - val_loss: 0.3815\n",
            "Epoch 4/35\n",
            "32/32 [==============================] - 17s 542ms/step - loss: 0.3634 - val_loss: 0.3491\n",
            "Epoch 5/35\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.3311 - val_loss: 0.3252\n",
            "Epoch 6/35\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.3001 - val_loss: 0.3146\n",
            "Epoch 7/35\n",
            "32/32 [==============================] - 17s 545ms/step - loss: 0.2612 - val_loss: 0.3577\n",
            "Epoch 8/35\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.2432 - val_loss: 0.3245\n",
            "[2026_02_12-14:25:57] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:25:57] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:25:57] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 570ms/step - loss: 0.2854 - val_loss: 0.3194\n",
            "[Exp6][G7_no_freeze][seed=44] AUC=0.8512, AUPRC=0.4986, F1=0.4043\n",
            "\n",
            "[Exp6] 配置汇总（按AUPRC均值排序）:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">AUC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">AUPRC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">F1</th>\n",
              "      <th colspan=\"2\" halign=\"left\">MCC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Brier</th>\n",
              "      <th colspan=\"2\" halign=\"left\">ECE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Config</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>G3_no_final_stage</th>\n",
              "      <td>0.890325</td>\n",
              "      <td>0.014937</td>\n",
              "      <td>0.616031</td>\n",
              "      <td>0.041537</td>\n",
              "      <td>0.441999</td>\n",
              "      <td>0.038072</td>\n",
              "      <td>0.408854</td>\n",
              "      <td>0.038001</td>\n",
              "      <td>0.061706</td>\n",
              "      <td>0.003975</td>\n",
              "      <td>0.064582</td>\n",
              "      <td>0.012240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G1_baseline_like</th>\n",
              "      <td>0.891834</td>\n",
              "      <td>0.010782</td>\n",
              "      <td>0.605903</td>\n",
              "      <td>0.030011</td>\n",
              "      <td>0.459245</td>\n",
              "      <td>0.051180</td>\n",
              "      <td>0.424317</td>\n",
              "      <td>0.046906</td>\n",
              "      <td>0.054627</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.026364</td>\n",
              "      <td>0.005923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G5_less_dropout</th>\n",
              "      <td>0.891272</td>\n",
              "      <td>0.011710</td>\n",
              "      <td>0.603010</td>\n",
              "      <td>0.011123</td>\n",
              "      <td>0.462408</td>\n",
              "      <td>0.052523</td>\n",
              "      <td>0.419006</td>\n",
              "      <td>0.048180</td>\n",
              "      <td>0.054965</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.028183</td>\n",
              "      <td>0.008401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G4_lower_unfrozen_lr</th>\n",
              "      <td>0.885178</td>\n",
              "      <td>0.010407</td>\n",
              "      <td>0.599872</td>\n",
              "      <td>0.026931</td>\n",
              "      <td>0.464697</td>\n",
              "      <td>0.056607</td>\n",
              "      <td>0.418950</td>\n",
              "      <td>0.045371</td>\n",
              "      <td>0.055539</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>0.029476</td>\n",
              "      <td>0.008106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G2_shorter_train</th>\n",
              "      <td>0.882840</td>\n",
              "      <td>0.014274</td>\n",
              "      <td>0.588060</td>\n",
              "      <td>0.034734</td>\n",
              "      <td>0.472370</td>\n",
              "      <td>0.054602</td>\n",
              "      <td>0.425530</td>\n",
              "      <td>0.055952</td>\n",
              "      <td>0.056027</td>\n",
              "      <td>0.002511</td>\n",
              "      <td>0.034296</td>\n",
              "      <td>0.010432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G6_final_len512</th>\n",
              "      <td>0.883935</td>\n",
              "      <td>0.023477</td>\n",
              "      <td>0.586385</td>\n",
              "      <td>0.043816</td>\n",
              "      <td>0.461105</td>\n",
              "      <td>0.063541</td>\n",
              "      <td>0.416464</td>\n",
              "      <td>0.062308</td>\n",
              "      <td>0.059555</td>\n",
              "      <td>0.006016</td>\n",
              "      <td>0.040916</td>\n",
              "      <td>0.011098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G7_no_freeze</th>\n",
              "      <td>0.825178</td>\n",
              "      <td>0.042472</td>\n",
              "      <td>0.473645</td>\n",
              "      <td>0.060857</td>\n",
              "      <td>0.416582</td>\n",
              "      <td>0.053550</td>\n",
              "      <td>0.362359</td>\n",
              "      <td>0.056201</td>\n",
              "      <td>0.065810</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>0.037198</td>\n",
              "      <td>0.006874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           AUC               AUPRC                  F1  \\\n",
              "                          mean       std      mean       std      mean   \n",
              "Config                                                                   \n",
              "G3_no_final_stage     0.890325  0.014937  0.616031  0.041537  0.441999   \n",
              "G1_baseline_like      0.891834  0.010782  0.605903  0.030011  0.459245   \n",
              "G5_less_dropout       0.891272  0.011710  0.603010  0.011123  0.462408   \n",
              "G4_lower_unfrozen_lr  0.885178  0.010407  0.599872  0.026931  0.464697   \n",
              "G2_shorter_train      0.882840  0.014274  0.588060  0.034734  0.472370   \n",
              "G6_final_len512       0.883935  0.023477  0.586385  0.043816  0.461105   \n",
              "G7_no_freeze          0.825178  0.042472  0.473645  0.060857  0.416582   \n",
              "\n",
              "                                     MCC               Brier            \\\n",
              "                           std      mean       std      mean       std   \n",
              "Config                                                                   \n",
              "G3_no_final_stage     0.038072  0.408854  0.038001  0.061706  0.003975   \n",
              "G1_baseline_like      0.051180  0.424317  0.046906  0.054627  0.001910   \n",
              "G5_less_dropout       0.052523  0.419006  0.048180  0.054965  0.000801   \n",
              "G4_lower_unfrozen_lr  0.056607  0.418950  0.045371  0.055539  0.002357   \n",
              "G2_shorter_train      0.054602  0.425530  0.055952  0.056027  0.002511   \n",
              "G6_final_len512       0.063541  0.416464  0.062308  0.059555  0.006016   \n",
              "G7_no_freeze          0.053550  0.362359  0.056201  0.065810  0.003756   \n",
              "\n",
              "                           ECE            \n",
              "                          mean       std  \n",
              "Config                                    \n",
              "G3_no_final_stage     0.064582  0.012240  \n",
              "G1_baseline_like      0.026364  0.005923  \n",
              "G5_less_dropout       0.028183  0.008401  \n",
              "G4_lower_unfrozen_lr  0.029476  0.008106  \n",
              "G2_shorter_train      0.034296  0.010432  \n",
              "G6_final_len512       0.040916  0.011098  \n",
              "G7_no_freeze          0.037198  0.006874  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp6] 基线门槛: AUC>0.8809 且 AUPRC>0.5818\n",
            "[Exp6] 通过门槛配置: ['G3_no_final_stage', 'G1_baseline_like', 'G5_less_dropout', 'G4_lower_unfrozen_lr', 'G2_shorter_train', 'G6_final_len512']\n",
            "[Exp6] 当前候选最优: G3_no_final_stage\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验6：ProteinBERT微调优化矩阵（阶段2）\n",
        "# 目标：系统搜索冻结/学习率/序列长度策略，并执行提升门槛\n",
        "# =====================================================================\n",
        "\n",
        "finetune_cfgs = [\n",
        "    dict(name='G1_baseline_like', dropout=0.5, seq_len=512, batch_size=32, max_epochs=40, lr=1e-4, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=1, final_seq_len=1024, final_lr=1e-5),\n",
        "    dict(name='G2_shorter_train', dropout=0.5, seq_len=512, batch_size=32, max_epochs=30, lr=1e-4, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=1, final_seq_len=1024, final_lr=1e-5),\n",
        "    dict(name='G3_no_final_stage', dropout=0.5, seq_len=512, batch_size=32, max_epochs=40, lr=1e-4, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=0, final_seq_len=1024, final_lr=1e-5),\n",
        "    dict(name='G4_lower_unfrozen_lr', dropout=0.5, seq_len=512, batch_size=32, max_epochs=35, lr=5e-5, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=1, final_seq_len=1024, final_lr=1e-5),\n",
        "    dict(name='G5_less_dropout', dropout=0.35, seq_len=512, batch_size=32, max_epochs=40, lr=1e-4, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=1, final_seq_len=1024, final_lr=1e-5),\n",
        "    dict(name='G6_final_len512', dropout=0.5, seq_len=512, batch_size=32, max_epochs=40, lr=1e-4, freeze_first=True,  lr_frozen=1e-2, n_final_epochs=1, final_seq_len=512,  final_lr=1e-5),\n",
        "    dict(name='G7_no_freeze', dropout=0.5, seq_len=512, batch_size=32, max_epochs=35, lr=5e-5, freeze_first=False, lr_frozen=1e-2, n_final_epochs=1, final_seq_len=1024, final_lr=1e-5),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for cfg in finetune_cfgs:\n",
        "    for seed in SEEDS:\n",
        "        tr_df, va_df = train_test_split(\n",
        "            full_train, test_size=0.1, stratify=full_train['label'], random_state=seed\n",
        "        )\n",
        "        _, _, met, _ = run_finetune_once(tr_df.reset_index(drop=True), va_df.reset_index(drop=True), full_test, cfg)\n",
        "        rows.append({**{'Config': cfg['name'], 'Seed': seed}, **met})\n",
        "        print(f\"[Exp6][{cfg['name']}][seed={seed}] AUC={met['AUC']:.4f}, AUPRC={met['AUPRC']:.4f}, F1={met['F1']:.4f}\")\n",
        "\n",
        "exp6_df = pd.DataFrame(rows)\n",
        "summary6_flat = exp6_df.groupby('Config')[['AUC','AUPRC','F1','MCC','Brier','ECE']].agg(['mean','std'])\n",
        "summary6_rank = summary6_flat.sort_values(('AUPRC', 'mean'), ascending=False)\n",
        "\n",
        "print('\\n[Exp6] 配置汇总（按AUPRC均值排序）:')\n",
        "display(summary6_rank)\n",
        "\n",
        "base_auc = float(BASELINE_RESULT['AUC'].mean())\n",
        "base_auprc = float(BASELINE_RESULT['AUPRC'].mean())\n",
        "\n",
        "accepted_cfgs = []\n",
        "for cfg_name in summary6_rank.index:\n",
        "    auc_m = float(summary6_rank.loc[cfg_name, ('AUC', 'mean')])\n",
        "    auprc_m = float(summary6_rank.loc[cfg_name, ('AUPRC', 'mean')])\n",
        "    if (auc_m > base_auc) and (auprc_m > base_auprc):\n",
        "        accepted_cfgs.append(cfg_name)\n",
        "\n",
        "if len(accepted_cfgs) == 0:\n",
        "    accepted_cfgs = [summary6_rank.index[0]]\n",
        "\n",
        "best_cfg_name = accepted_cfgs[0]\n",
        "BEST_FINETUNE_CFG = [c for c in finetune_cfgs if c['name'] == best_cfg_name][0]\n",
        "TOP_CFG_NAMES = accepted_cfgs[:3]\n",
        "\n",
        "print(f\"[Exp6] 基线门槛: AUC>{base_auc:.4f} 且 AUPRC>{base_auprc:.4f}\")\n",
        "print(f\"[Exp6] 通过门槛配置: {accepted_cfgs}\")\n",
        "print(f\"[Exp6] 当前候选最优: {best_cfg_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026_02_12-14:26:48] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:26:48] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:26:48] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 213ms/step - loss: 0.4556 - val_loss: 0.3751\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2659 - val_loss: 0.3543\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2369 - val_loss: 0.3352\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2202 - val_loss: 0.3099\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2157 - val_loss: 0.3064\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.1830 - val_loss: 0.3420\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.1567 - val_loss: 0.3170\n",
            "[2026_02_12-14:27:35] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:27:40] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 582ms/step - loss: 0.1827 - val_loss: 0.3186\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 553ms/step - loss: 0.1530 - val_loss: 0.3553\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.1227 - val_loss: 0.3358\n",
            "[Exp7][G3_no_final_stage][none][seed=0] AUC=0.8862, AUPRC=0.6163, F1=0.5217\n",
            "[Exp7][G3_no_final_stage][platt][seed=0] AUC=0.8862, AUPRC=0.6163, F1=0.4839\n",
            "[Exp7][G3_no_final_stage][isotonic][seed=0] AUC=0.8888, AUPRC=0.5807, F1=0.5079\n",
            "[2026_02_12-14:28:47] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:28:47] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:28:47] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 210ms/step - loss: 0.4464 - val_loss: 0.2922\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.3047 - val_loss: 0.2614\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2626 - val_loss: 0.2576\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2184 - val_loss: 0.2547\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.2137 - val_loss: 0.2904\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2023 - val_loss: 0.2576\n",
            "[2026_02_12-14:29:28] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:29:33] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 580ms/step - loss: 0.2157 - val_loss: 0.2364\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1831 - val_loss: 0.2384\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.1393 - val_loss: 0.2319\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1352 - val_loss: 0.2373\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 18s 557ms/step - loss: 0.1148 - val_loss: 0.2353\n",
            "[Exp7][G3_no_final_stage][none][seed=11] AUC=0.8964, AUPRC=0.6308, F1=0.4396\n",
            "[Exp7][G3_no_final_stage][platt][seed=11] AUC=0.8964, AUPRC=0.6308, F1=0.4396\n",
            "[Exp7][G3_no_final_stage][isotonic][seed=11] AUC=0.8922, AUPRC=0.6049, F1=0.4396\n",
            "[2026_02_12-14:31:14] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:31:14] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:31:14] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 209ms/step - loss: 0.4702 - val_loss: 0.2930\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2691 - val_loss: 0.2779\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2511 - val_loss: 0.2670\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2558 - val_loss: 0.2836\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2220 - val_loss: 0.2728\n",
            "[2026_02_12-14:31:49] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:31:54] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 575ms/step - loss: 0.2159 - val_loss: 0.2726\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1884 - val_loss: 0.2778\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 542ms/step - loss: 0.1490 - val_loss: 0.2889\n",
            "[Exp7][G3_no_final_stage][none][seed=22] AUC=0.8825, AUPRC=0.5993, F1=0.5085\n",
            "[Exp7][G3_no_final_stage][platt][seed=22] AUC=0.8825, AUPRC=0.5993, F1=0.4638\n",
            "[Exp7][G3_no_final_stage][isotonic][seed=22] AUC=0.8816, AUPRC=0.5547, F1=0.5085\n",
            "[2026_02_12-14:33:00] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:33:00] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:33:00] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 240ms/step - loss: 0.4512 - val_loss: 0.3853\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2743 - val_loss: 0.3813\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2396 - val_loss: 0.3884\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2169 - val_loss: 0.3852\n",
            "[2026_02_12-14:33:29] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:33:34] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 592ms/step - loss: 0.2356 - val_loss: 0.3768\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 548ms/step - loss: 0.1914 - val_loss: 0.3980\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 555ms/step - loss: 0.1622 - val_loss: 0.3950\n",
            "[Exp7][G3_no_final_stage][none][seed=33] AUC=0.8811, AUPRC=0.5668, F1=0.3810\n",
            "[Exp7][G3_no_final_stage][platt][seed=33] AUC=0.8811, AUPRC=0.5668, F1=0.3750\n",
            "[Exp7][G3_no_final_stage][isotonic][seed=33] AUC=0.8578, AUPRC=0.4751, F1=0.3846\n",
            "[2026_02_12-14:34:42] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:34:42] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:34:42] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 10s 207ms/step - loss: 0.4556 - val_loss: 0.3378\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.2702 - val_loss: 0.3422\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2249 - val_loss: 0.3391\n",
            "[2026_02_12-14:35:05] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:35:10] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 575ms/step - loss: 0.2863 - val_loss: 0.3354\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 550ms/step - loss: 0.2768 - val_loss: 0.3271\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 545ms/step - loss: 0.2286 - val_loss: 0.3168\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.2028 - val_loss: 0.3140\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 538ms/step - loss: 0.1694 - val_loss: 0.3101\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.1239 - val_loss: 0.3286\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 17s 537ms/step - loss: 0.1013 - val_loss: 0.3645\n",
            "[Exp7][G3_no_final_stage][none][seed=44] AUC=0.8655, AUPRC=0.5363, F1=0.4063\n",
            "[Exp7][G3_no_final_stage][platt][seed=44] AUC=0.8655, AUPRC=0.5363, F1=0.4037\n",
            "[Exp7][G3_no_final_stage][isotonic][seed=44] AUC=0.8447, AUPRC=0.4480, F1=0.4063\n",
            "[2026_02_12-14:37:25] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:37:25] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:37:25] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 207ms/step - loss: 0.4611 - val_loss: 0.4408\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.3752 - val_loss: 0.4182\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.3073 - val_loss: 0.3714\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2368 - val_loss: 0.3373\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2131 - val_loss: 0.3312\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.1891 - val_loss: 0.3192\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2044 - val_loss: 0.3502\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1761 - val_loss: 0.3293\n",
            "[2026_02_12-14:38:17] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:38:22] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 573ms/step - loss: 0.1811 - val_loss: 0.3259\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1475 - val_loss: 0.3455\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 543ms/step - loss: 0.1198 - val_loss: 0.3470\n",
            "[2026_02_12-14:39:19] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:39:19] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:39:19] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 570ms/step - loss: 0.1878 - val_loss: 0.3373\n",
            "[Exp7][G1_baseline_like][none][seed=0] AUC=0.8981, AUPRC=0.6069, F1=0.5098\n",
            "[Exp7][G1_baseline_like][platt][seed=0] AUC=0.8981, AUPRC=0.6069, F1=0.5385\n",
            "[Exp7][G1_baseline_like][isotonic][seed=0] AUC=0.9066, AUPRC=0.5096, F1=0.5385\n",
            "[2026_02_12-14:40:14] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:40:14] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:40:14] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 233ms/step - loss: 0.5408 - val_loss: 0.3133\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.3020 - val_loss: 0.2663\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2436 - val_loss: 0.2654\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2451 - val_loss: 0.2501\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2227 - val_loss: 0.2556\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.1960 - val_loss: 0.2537\n",
            "[2026_02_12-14:40:54] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:40:59] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 566ms/step - loss: 0.2098 - val_loss: 0.2701\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 538ms/step - loss: 0.1786 - val_loss: 0.2415\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 537ms/step - loss: 0.1518 - val_loss: 0.2388\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 536ms/step - loss: 0.1147 - val_loss: 0.2403\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 535ms/step - loss: 0.0857 - val_loss: 0.2479\n",
            "[2026_02_12-14:42:30] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:42:30] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:42:30] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 558ms/step - loss: 0.1549 - val_loss: 0.2326\n",
            "[Exp7][G1_baseline_like][none][seed=11] AUC=0.8950, AUPRC=0.6087, F1=0.4419\n",
            "[Exp7][G1_baseline_like][platt][seed=11] AUC=0.8950, AUPRC=0.6087, F1=0.4444\n",
            "[Exp7][G1_baseline_like][isotonic][seed=11] AUC=0.8891, AUPRC=0.5721, F1=0.3958\n",
            "[2026_02_12-14:43:23] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:43:23] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:43:23] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 205ms/step - loss: 0.4509 - val_loss: 0.4170\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.3483 - val_loss: 0.3315\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2726 - val_loss: 0.2770\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2424 - val_loss: 0.2865\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 173ms/step - loss: 0.2086 - val_loss: 0.2862\n",
            "[2026_02_12-14:43:58] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:44:03] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 577ms/step - loss: 0.2249 - val_loss: 0.2756\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 543ms/step - loss: 0.1791 - val_loss: 0.2844\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 542ms/step - loss: 0.1498 - val_loss: 0.2856\n",
            "[2026_02_12-14:45:00] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:45:00] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:45:00] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 556ms/step - loss: 0.2085 - val_loss: 0.2909\n",
            "[Exp7][G1_baseline_like][none][seed=22] AUC=0.8763, AUPRC=0.5797, F1=0.4048\n",
            "[Exp7][G1_baseline_like][platt][seed=22] AUC=0.8763, AUPRC=0.5797, F1=0.4048\n",
            "[Exp7][G1_baseline_like][isotonic][seed=22] AUC=0.8669, AUPRC=0.5365, F1=0.3953\n",
            "[2026_02_12-14:45:53] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:45:53] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:45:53] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.5161 - val_loss: 0.4376\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.3453 - val_loss: 0.4365\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.2588 - val_loss: 0.3732\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2295 - val_loss: 0.3880\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.1930 - val_loss: 0.3923\n",
            "[2026_02_12-14:46:29] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:46:34] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 583ms/step - loss: 0.2280 - val_loss: 0.3786\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1809 - val_loss: 0.3955\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 548ms/step - loss: 0.1426 - val_loss: 0.4032\n",
            "[2026_02_12-14:47:31] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:47:31] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:47:31] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 577ms/step - loss: 0.2167 - val_loss: 0.3622\n",
            "[Exp7][G1_baseline_like][none][seed=33] AUC=0.8682, AUPRC=0.5318, F1=0.4308\n",
            "[Exp7][G1_baseline_like][platt][seed=33] AUC=0.8682, AUPRC=0.5318, F1=0.4375\n",
            "[Exp7][G1_baseline_like][isotonic][seed=33] AUC=0.8345, AUPRC=0.4566, F1=0.4375\n",
            "[2026_02_12-14:48:27] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:48:27] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:48:27] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 217ms/step - loss: 0.4606 - val_loss: 0.3551\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.2877 - val_loss: 0.3372\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.2451 - val_loss: 0.3418\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.2142 - val_loss: 0.3323\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 188ms/step - loss: 0.2071 - val_loss: 0.3357\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2087 - val_loss: 0.3295\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.1957 - val_loss: 0.3295\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.1988 - val_loss: 0.3293\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2172 - val_loss: 0.3286\n",
            "Epoch 10/40\n",
            "32/32 [==============================] - 6s 194ms/step - loss: 0.1896 - val_loss: 0.3283\n",
            "Epoch 11/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2015 - val_loss: 0.3275\n",
            "Epoch 12/40\n",
            "32/32 [==============================] - 6s 191ms/step - loss: 0.2048 - val_loss: 0.3276\n",
            "Epoch 13/40\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 0.1904 - val_loss: 0.3275\n",
            "Epoch 14/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2039 - val_loss: 0.3275\n",
            "Epoch 15/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.2060 - val_loss: 0.3275\n",
            "Epoch 16/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2081 - val_loss: 0.3274\n",
            "Epoch 17/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2095 - val_loss: 0.3274\n",
            "Epoch 18/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.2114 - val_loss: 0.3274\n",
            "Epoch 19/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2073 - val_loss: 0.3274\n",
            "Epoch 20/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.1931 - val_loss: 0.3274\n",
            "[2026_02_12-14:50:31] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:50:36] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 574ms/step - loss: 0.2072 - val_loss: 0.3270\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1683 - val_loss: 0.3318\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1309 - val_loss: 0.3360\n",
            "[2026_02_12-14:51:34] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:51:34] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:51:34] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 41s 587ms/step - loss: 0.1931 - val_loss: 0.3208\n",
            "[Exp7][G1_baseline_like][none][seed=44] AUC=0.8729, AUPRC=0.5920, F1=0.4375\n",
            "[Exp7][G1_baseline_like][platt][seed=44] AUC=0.8729, AUPRC=0.5920, F1=0.4375\n",
            "[Exp7][G1_baseline_like][isotonic][seed=44] AUC=0.8396, AUPRC=0.3525, F1=0.4375\n",
            "[2026_02_12-14:52:29] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:52:29] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:52:29] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.4721 - val_loss: 0.3370\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2659 - val_loss: 0.3351\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 183ms/step - loss: 0.2276 - val_loss: 0.5566\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 185ms/step - loss: 0.3156 - val_loss: 0.3309\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2170 - val_loss: 0.3221\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1814 - val_loss: 0.3274\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 190ms/step - loss: 0.1778 - val_loss: 0.3197\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.1764 - val_loss: 0.3210\n",
            "Epoch 9/40\n",
            "32/32 [==============================] - 6s 187ms/step - loss: 0.1804 - val_loss: 0.3216\n",
            "[2026_02_12-14:53:27] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:53:33] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 577ms/step - loss: 0.1806 - val_loss: 0.3412\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 535ms/step - loss: 0.1453 - val_loss: 0.3625\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 550ms/step - loss: 0.1015 - val_loss: 0.3361\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.0791 - val_loss: 0.3573\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 536ms/step - loss: 0.0667 - val_loss: 0.3602\n",
            "[2026_02_12-14:55:04] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:55:04] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:55:04] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 565ms/step - loss: 0.1204 - val_loss: 0.3516\n",
            "[Exp7][G5_less_dropout][none][seed=0] AUC=0.8849, AUPRC=0.5868, F1=0.3918\n",
            "[Exp7][G5_less_dropout][platt][seed=0] AUC=0.8849, AUPRC=0.5868, F1=0.3918\n",
            "[Exp7][G5_less_dropout][isotonic][seed=0] AUC=0.8768, AUPRC=0.5015, F1=0.3918\n",
            "[2026_02_12-14:55:59] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:55:59] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:55:59] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 206ms/step - loss: 0.5016 - val_loss: 0.3557\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.3040 - val_loss: 0.2870\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.2508 - val_loss: 0.2536\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2107 - val_loss: 0.2460\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.1795 - val_loss: 0.2574\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.1599 - val_loss: 0.2460\n",
            "[2026_02_12-14:56:39] Training the entire fine-tuned model...\n",
            "[2026_02_12-14:56:44] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 603ms/step - loss: 0.1909 - val_loss: 0.2445\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 538ms/step - loss: 0.1583 - val_loss: 0.2505\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 538ms/step - loss: 0.1297 - val_loss: 0.2369\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1008 - val_loss: 0.2344\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.0911 - val_loss: 0.2405\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.0798 - val_loss: 0.2388\n",
            "[2026_02_12-14:58:34] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-14:58:34] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-14:58:34] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 562ms/step - loss: 0.1266 - val_loss: 0.2324\n",
            "[Exp7][G5_less_dropout][none][seed=11] AUC=0.9109, AUPRC=0.6134, F1=0.5106\n",
            "[Exp7][G5_less_dropout][platt][seed=11] AUC=0.9109, AUPRC=0.6134, F1=0.4235\n",
            "[Exp7][G5_less_dropout][isotonic][seed=11] AUC=0.8969, AUPRC=0.5773, F1=0.4857\n",
            "[2026_02_12-14:59:26] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:59:26] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-14:59:26] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 219ms/step - loss: 0.4727 - val_loss: 0.3207\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.3137 - val_loss: 0.3607\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2378 - val_loss: 0.2891\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2426 - val_loss: 0.2745\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2111 - val_loss: 0.2700\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2100 - val_loss: 0.2816\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 189ms/step - loss: 0.1843 - val_loss: 0.2710\n",
            "[2026_02_12-15:00:13] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:00:18] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 23s 568ms/step - loss: 0.2074 - val_loss: 0.2999\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 533ms/step - loss: 0.1714 - val_loss: 0.2776\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1264 - val_loss: 0.4245\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 550ms/step - loss: 0.0813 - val_loss: 0.3513\n",
            "[2026_02_12-15:01:32] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:01:32] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:01:32] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 570ms/step - loss: 0.1599 - val_loss: 0.2816\n",
            "[Exp7][G5_less_dropout][none][seed=22] AUC=0.8914, AUPRC=0.6178, F1=0.5373\n",
            "[Exp7][G5_less_dropout][platt][seed=22] AUC=0.8914, AUPRC=0.6178, F1=0.5217\n",
            "[Exp7][G5_less_dropout][isotonic][seed=22] AUC=0.8823, AUPRC=0.5118, F1=0.5217\n",
            "[2026_02_12-15:02:26] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:02:26] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:02:26] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 211ms/step - loss: 0.4682 - val_loss: 0.3768\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2727 - val_loss: 0.3769\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2092 - val_loss: 0.3923\n",
            "[2026_02_12-15:02:50] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:02:55] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 575ms/step - loss: 0.2737 - val_loss: 0.3696\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 550ms/step - loss: 0.2437 - val_loss: 0.3631\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 559ms/step - loss: 0.1974 - val_loss: 0.4010\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1554 - val_loss: 0.4117\n",
            "[2026_02_12-15:04:10] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:04:10] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:04:10] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 568ms/step - loss: 0.2276 - val_loss: 0.3539\n",
            "[Exp7][G5_less_dropout][none][seed=33] AUC=0.8740, AUPRC=0.5865, F1=0.3607\n",
            "[Exp7][G5_less_dropout][platt][seed=33] AUC=0.8740, AUPRC=0.5865, F1=0.3818\n",
            "[Exp7][G5_less_dropout][isotonic][seed=33] AUC=0.8621, AUPRC=0.5165, F1=0.3607\n",
            "[2026_02_12-15:05:05] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:05:05] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:05:05] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 210ms/step - loss: 0.4782 - val_loss: 0.3675\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.3009 - val_loss: 0.3508\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 179ms/step - loss: 0.2191 - val_loss: 0.3411\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 180ms/step - loss: 0.2143 - val_loss: 0.3333\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 186ms/step - loss: 0.1968 - val_loss: 0.3423\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 182ms/step - loss: 0.1627 - val_loss: 0.3421\n",
            "[2026_02_12-15:05:46] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:05:51] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 579ms/step - loss: 0.1725 - val_loss: 0.3414\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 545ms/step - loss: 0.1410 - val_loss: 0.3744\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.0949 - val_loss: 0.3590\n",
            "[2026_02_12-15:06:48] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:06:48] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:06:49] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 573ms/step - loss: 0.1666 - val_loss: 0.3092\n",
            "[Exp7][G5_less_dropout][none][seed=44] AUC=0.8928, AUPRC=0.5909, F1=0.4179\n",
            "[Exp7][G5_less_dropout][platt][seed=44] AUC=0.8928, AUPRC=0.5909, F1=0.4444\n",
            "[Exp7][G5_less_dropout][isotonic][seed=44] AUC=0.8874, AUPRC=0.5354, F1=0.4179\n",
            "\n",
            "[Exp7] 校准结果汇总（按AUPRC均值排序）:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">AUC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">AUPRC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">F1</th>\n",
              "      <th colspan=\"2\" halign=\"left\">MCC</th>\n",
              "      <th colspan=\"2\" halign=\"left\">Brier</th>\n",
              "      <th colspan=\"2\" halign=\"left\">ECE</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Config</th>\n",
              "      <th>Calib</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">G5_less_dropout</th>\n",
              "      <th>none</th>\n",
              "      <td>0.890799</td>\n",
              "      <td>0.013493</td>\n",
              "      <td>0.599071</td>\n",
              "      <td>0.015265</td>\n",
              "      <td>0.443654</td>\n",
              "      <td>0.076655</td>\n",
              "      <td>0.403499</td>\n",
              "      <td>0.073259</td>\n",
              "      <td>0.055554</td>\n",
              "      <td>0.002190</td>\n",
              "      <td>0.027581</td>\n",
              "      <td>0.007526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>platt</th>\n",
              "      <td>0.890799</td>\n",
              "      <td>0.013493</td>\n",
              "      <td>0.599071</td>\n",
              "      <td>0.015265</td>\n",
              "      <td>0.432657</td>\n",
              "      <td>0.055713</td>\n",
              "      <td>0.390972</td>\n",
              "      <td>0.051436</td>\n",
              "      <td>0.147471</td>\n",
              "      <td>0.016247</td>\n",
              "      <td>0.302451</td>\n",
              "      <td>0.027724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">G3_no_final_stage</th>\n",
              "      <th>none</th>\n",
              "      <td>0.882367</td>\n",
              "      <td>0.011163</td>\n",
              "      <td>0.589878</td>\n",
              "      <td>0.038292</td>\n",
              "      <td>0.451395</td>\n",
              "      <td>0.061941</td>\n",
              "      <td>0.407201</td>\n",
              "      <td>0.061980</td>\n",
              "      <td>0.062586</td>\n",
              "      <td>0.008494</td>\n",
              "      <td>0.052371</td>\n",
              "      <td>0.010012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>platt</th>\n",
              "      <td>0.882367</td>\n",
              "      <td>0.011163</td>\n",
              "      <td>0.589878</td>\n",
              "      <td>0.038292</td>\n",
              "      <td>0.433174</td>\n",
              "      <td>0.044164</td>\n",
              "      <td>0.397477</td>\n",
              "      <td>0.030728</td>\n",
              "      <td>0.140135</td>\n",
              "      <td>0.018265</td>\n",
              "      <td>0.286485</td>\n",
              "      <td>0.031194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">G1_baseline_like</th>\n",
              "      <th>none</th>\n",
              "      <td>0.882101</td>\n",
              "      <td>0.013524</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.031382</td>\n",
              "      <td>0.444939</td>\n",
              "      <td>0.039010</td>\n",
              "      <td>0.393941</td>\n",
              "      <td>0.042079</td>\n",
              "      <td>0.056667</td>\n",
              "      <td>0.003271</td>\n",
              "      <td>0.026243</td>\n",
              "      <td>0.009001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>platt</th>\n",
              "      <td>0.882101</td>\n",
              "      <td>0.013524</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.031382</td>\n",
              "      <td>0.452534</td>\n",
              "      <td>0.050456</td>\n",
              "      <td>0.400794</td>\n",
              "      <td>0.053729</td>\n",
              "      <td>0.148975</td>\n",
              "      <td>0.016763</td>\n",
              "      <td>0.302825</td>\n",
              "      <td>0.025253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G3_no_final_stage</th>\n",
              "      <th>isotonic</th>\n",
              "      <td>0.873033</td>\n",
              "      <td>0.020775</td>\n",
              "      <td>0.532675</td>\n",
              "      <td>0.068018</td>\n",
              "      <td>0.449367</td>\n",
              "      <td>0.057167</td>\n",
              "      <td>0.403663</td>\n",
              "      <td>0.055256</td>\n",
              "      <td>0.068062</td>\n",
              "      <td>0.006491</td>\n",
              "      <td>0.063038</td>\n",
              "      <td>0.009169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G5_less_dropout</th>\n",
              "      <th>isotonic</th>\n",
              "      <td>0.881124</td>\n",
              "      <td>0.012931</td>\n",
              "      <td>0.528485</td>\n",
              "      <td>0.029905</td>\n",
              "      <td>0.435554</td>\n",
              "      <td>0.066677</td>\n",
              "      <td>0.393867</td>\n",
              "      <td>0.061348</td>\n",
              "      <td>0.067247</td>\n",
              "      <td>0.005726</td>\n",
              "      <td>0.063804</td>\n",
              "      <td>0.014645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G1_baseline_like</th>\n",
              "      <th>isotonic</th>\n",
              "      <td>0.867322</td>\n",
              "      <td>0.031059</td>\n",
              "      <td>0.485454</td>\n",
              "      <td>0.085427</td>\n",
              "      <td>0.440929</td>\n",
              "      <td>0.058411</td>\n",
              "      <td>0.390080</td>\n",
              "      <td>0.058853</td>\n",
              "      <td>0.074384</td>\n",
              "      <td>0.011676</td>\n",
              "      <td>0.071693</td>\n",
              "      <td>0.019054</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 AUC               AUPRC                  F1  \\\n",
              "                                mean       std      mean       std      mean   \n",
              "Config            Calib                                                        \n",
              "G5_less_dropout   none      0.890799  0.013493  0.599071  0.015265  0.443654   \n",
              "                  platt     0.890799  0.013493  0.599071  0.015265  0.432657   \n",
              "G3_no_final_stage none      0.882367  0.011163  0.589878  0.038292  0.451395   \n",
              "                  platt     0.882367  0.011163  0.589878  0.038292  0.433174   \n",
              "G1_baseline_like  none      0.882101  0.013524  0.583824  0.031382  0.444939   \n",
              "                  platt     0.882101  0.013524  0.583824  0.031382  0.452534   \n",
              "G3_no_final_stage isotonic  0.873033  0.020775  0.532675  0.068018  0.449367   \n",
              "G5_less_dropout   isotonic  0.881124  0.012931  0.528485  0.029905  0.435554   \n",
              "G1_baseline_like  isotonic  0.867322  0.031059  0.485454  0.085427  0.440929   \n",
              "\n",
              "                                           MCC               Brier            \\\n",
              "                                 std      mean       std      mean       std   \n",
              "Config            Calib                                                        \n",
              "G5_less_dropout   none      0.076655  0.403499  0.073259  0.055554  0.002190   \n",
              "                  platt     0.055713  0.390972  0.051436  0.147471  0.016247   \n",
              "G3_no_final_stage none      0.061941  0.407201  0.061980  0.062586  0.008494   \n",
              "                  platt     0.044164  0.397477  0.030728  0.140135  0.018265   \n",
              "G1_baseline_like  none      0.039010  0.393941  0.042079  0.056667  0.003271   \n",
              "                  platt     0.050456  0.400794  0.053729  0.148975  0.016763   \n",
              "G3_no_final_stage isotonic  0.057167  0.403663  0.055256  0.068062  0.006491   \n",
              "G5_less_dropout   isotonic  0.066677  0.393867  0.061348  0.067247  0.005726   \n",
              "G1_baseline_like  isotonic  0.058411  0.390080  0.058853  0.074384  0.011676   \n",
              "\n",
              "                                 ECE            \n",
              "                                mean       std  \n",
              "Config            Calib                         \n",
              "G5_less_dropout   none      0.027581  0.007526  \n",
              "                  platt     0.302451  0.027724  \n",
              "G3_no_final_stage none      0.052371  0.010012  \n",
              "                  platt     0.286485  0.031194  \n",
              "G1_baseline_like  none      0.026243  0.009001  \n",
              "                  platt     0.302825  0.025253  \n",
              "G3_no_final_stage isotonic  0.063038  0.009169  \n",
              "G5_less_dropout   isotonic  0.063804  0.014645  \n",
              "G1_baseline_like  isotonic  0.071693  0.019054  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp7] 最佳校准组合: cfg=G5_less_dropout, calib=none\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验7：概率校准（阶段4）\n",
        "# 目标：对最佳2-3个候选执行 Platt / Isotonic 校准，观察AUC/AUPRC/F1稳定性\n",
        "# =====================================================================\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "cfg_map = {c['name']: c for c in finetune_cfgs}\n",
        "calib_methods = ['none', 'platt', 'isotonic']\n",
        "\n",
        "rows = []\n",
        "for cfg_name in TOP_CFG_NAMES:\n",
        "    cfg = cfg_map[cfg_name]\n",
        "    for seed in SEEDS:\n",
        "        tr_df, va_df = train_test_split(\n",
        "            full_train, test_size=0.1, stratify=full_train['label'], random_state=seed\n",
        "        )\n",
        "        mg, ie, _, (yt_true, yt_prob) = run_finetune_once(\n",
        "            tr_df.reset_index(drop=True), va_df.reset_index(drop=True), full_test, cfg\n",
        "        )\n",
        "        yv_true, yv_prob = predict_proteinbert_probs(mg, ie, va_df['seq'], va_df['label'])\n",
        "\n",
        "        for method in calib_methods:\n",
        "            if method == 'none':\n",
        "                calib_valid = yv_prob\n",
        "                calib_test = yt_prob\n",
        "            elif method == 'platt':\n",
        "                platt = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)\n",
        "                platt.fit(yv_prob.reshape(-1, 1), yv_true.astype(int))\n",
        "                calib_valid = platt.predict_proba(yv_prob.reshape(-1, 1))[:, 1]\n",
        "                calib_test = platt.predict_proba(yt_prob.reshape(-1, 1))[:, 1]\n",
        "            else:\n",
        "                iso = IsotonicRegression(out_of_bounds='clip')\n",
        "                iso.fit(yv_prob, yv_true.astype(int))\n",
        "                calib_valid = iso.predict(yv_prob)\n",
        "                calib_test = iso.predict(yt_prob)\n",
        "\n",
        "            thr, _ = select_best_threshold(yv_true, calib_valid)\n",
        "            met = summarize_metrics(yt_true, calib_test, thr)\n",
        "            rows.append({'Config': cfg_name, 'Calib': method, 'Seed': seed, **met})\n",
        "            print(f\"[Exp7][{cfg_name}][{method}][seed={seed}] AUC={met['AUC']:.4f}, AUPRC={met['AUPRC']:.4f}, F1={met['F1']:.4f}\")\n",
        "\n",
        "exp7_df = pd.DataFrame(rows)\n",
        "summary7 = exp7_df.groupby(['Config', 'Calib'])[['AUC','AUPRC','F1','MCC','Brier','ECE']].agg(['mean','std'])\n",
        "summary7_rank = summary7.sort_values(('AUPRC', 'mean'), ascending=False)\n",
        "\n",
        "print('\\n[Exp7] 校准结果汇总（按AUPRC均值排序）:')\n",
        "display(summary7_rank)\n",
        "\n",
        "best_idx = summary7_rank.index[0]\n",
        "BEST_CALIB_CONFIG = best_idx[0]\n",
        "BEST_CALIB_METHOD = best_idx[1]\n",
        "BEST_CALIB_SUMMARY = summary7_rank\n",
        "\n",
        "print(f\"[Exp7] 最佳校准组合: cfg={BEST_CALIB_CONFIG}, calib={BEST_CALIB_METHOD}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2026_02_12-15:07:43] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:07:43] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:07:43] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 207ms/step - loss: 0.4912 - val_loss: 0.3489\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2855 - val_loss: 0.3214\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2420 - val_loss: 0.3406\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1869 - val_loss: 0.3223\n",
            "[2026_02_12-15:08:12] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:08:17] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 565ms/step - loss: 0.2293 - val_loss: 0.3205\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 18s 547ms/step - loss: 0.1840 - val_loss: 0.3383\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.1391 - val_loss: 0.3501\n",
            "[2026_02_12-15:09:14] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:09:14] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:09:15] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 564ms/step - loss: 0.2156 - val_loss: 0.3264\n",
            "[2026_02_12-15:10:08] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:10:08] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:10:08] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.5201 - val_loss: 0.3481\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.3399 - val_loss: 0.2892\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.2693 - val_loss: 0.2468\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 184ms/step - loss: 0.2115 - val_loss: 0.2687\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 181ms/step - loss: 0.1890 - val_loss: 0.2513\n",
            "[2026_02_12-15:10:44] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:10:49] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 568ms/step - loss: 0.2150 - val_loss: 0.2372\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 546ms/step - loss: 0.1818 - val_loss: 0.2325\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 18s 551ms/step - loss: 0.1415 - val_loss: 0.2359\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.1070 - val_loss: 0.2162\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 539ms/step - loss: 0.0863 - val_loss: 0.2167\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 17s 539ms/step - loss: 0.0699 - val_loss: 0.2137\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 18s 552ms/step - loss: 0.0698 - val_loss: 0.2189\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 17s 544ms/step - loss: 0.0632 - val_loss: 0.2197\n",
            "[2026_02_12-15:13:13] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:13:13] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:13:13] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 562ms/step - loss: 0.1169 - val_loss: 0.2085\n",
            "[2026_02_12-15:14:06] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:14:06] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:14:06] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 216ms/step - loss: 0.4776 - val_loss: 0.3174\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.3022 - val_loss: 0.2843\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2461 - val_loss: 0.3063\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2081 - val_loss: 0.2808\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 6s 178ms/step - loss: 0.2012 - val_loss: 0.2837\n",
            "Epoch 6/40\n",
            "32/32 [==============================] - 6s 175ms/step - loss: 0.2017 - val_loss: 0.2780\n",
            "Epoch 7/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2068 - val_loss: 0.2823\n",
            "Epoch 8/40\n",
            "32/32 [==============================] - 5s 172ms/step - loss: 0.1957 - val_loss: 0.2826\n",
            "[2026_02_12-15:14:58] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:15:03] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 21s 549ms/step - loss: 0.2007 - val_loss: 0.2786\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 534ms/step - loss: 0.1671 - val_loss: 0.2774\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 529ms/step - loss: 0.1244 - val_loss: 0.2957\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 520ms/step - loss: 0.0876 - val_loss: 0.3336\n",
            "[2026_02_12-15:16:15] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:16:15] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:16:15] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 554ms/step - loss: 0.1695 - val_loss: 0.2854\n",
            "[2026_02_12-15:17:07] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:17:08] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:17:08] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 213ms/step - loss: 0.4235 - val_loss: 0.3662\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 5s 172ms/step - loss: 0.2919 - val_loss: 0.3823\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 6s 177ms/step - loss: 0.2134 - val_loss: 0.3827\n",
            "[2026_02_12-15:17:31] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:17:36] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 21s 556ms/step - loss: 0.2754 - val_loss: 0.3618\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 525ms/step - loss: 0.2363 - val_loss: 0.3698\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 537ms/step - loss: 0.1994 - val_loss: 0.3726\n",
            "[2026_02_12-15:18:31] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:18:31] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:18:31] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 39s 558ms/step - loss: 0.2587 - val_loss: 0.3577\n",
            "[2026_02_12-15:19:24] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:19:25] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 510.\n",
            "[2026_02_12-15:19:25] Training with frozen pretrained layers...\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 11s 205ms/step - loss: 0.4658 - val_loss: 0.3568\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 6s 174ms/step - loss: 0.2675 - val_loss: 0.3305\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 5s 171ms/step - loss: 0.2458 - val_loss: 0.3358\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 6s 176ms/step - loss: 0.1949 - val_loss: 0.3341\n",
            "[2026_02_12-15:19:53] Training the entire fine-tuned model...\n",
            "[2026_02_12-15:19:58] Incompatible number of optimizer weights - will not initialize them.\n",
            "Epoch 1/40\n",
            "32/32 [==============================] - 22s 573ms/step - loss: 0.2362 - val_loss: 0.3484\n",
            "Epoch 2/40\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.1953 - val_loss: 0.3235\n",
            "Epoch 3/40\n",
            "32/32 [==============================] - 17s 538ms/step - loss: 0.1644 - val_loss: 0.3217\n",
            "Epoch 4/40\n",
            "32/32 [==============================] - 17s 541ms/step - loss: 0.1204 - val_loss: 0.3433\n",
            "Epoch 5/40\n",
            "32/32 [==============================] - 17s 547ms/step - loss: 0.0837 - val_loss: 0.3562\n",
            "[2026_02_12-15:21:29] Training on final epochs of sequence length 1024...\n",
            "[2026_02_12-15:21:29] Training set: Filtered out 0 of 996 (0.0%) records of lengths exceeding 1022.\n",
            "[2026_02_12-15:21:30] Validation set: Filtered out 0 of 111 (0.0%) records of lengths exceeding 1022.\n",
            "63/63 [==============================] - 40s 563ms/step - loss: 0.1671 - val_loss: 0.3199\n",
            "[Exp8] 最终结果对照:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>AUC</th>\n",
              "      <th>AUPRC</th>\n",
              "      <th>F1</th>\n",
              "      <th>MCC</th>\n",
              "      <th>Brier</th>\n",
              "      <th>ECE</th>\n",
              "      <th>Threshold</th>\n",
              "      <th>AUC_CI95</th>\n",
              "      <th>AUPRC_CI95</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline(multi-seed mean)</td>\n",
              "      <td>0.880917</td>\n",
              "      <td>0.581821</td>\n",
              "      <td>0.428205</td>\n",
              "      <td>0.388836</td>\n",
              "      <td>0.057129</td>\n",
              "      <td>0.031307</td>\n",
              "      <td>0.170</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Best single (G5_less_dropout+none)</td>\n",
              "      <td>0.881095</td>\n",
              "      <td>0.572910</td>\n",
              "      <td>0.438102</td>\n",
              "      <td>0.393739</td>\n",
              "      <td>0.057830</td>\n",
              "      <td>0.032222</td>\n",
              "      <td>0.175</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SeedEnsemble (G5_less_dropout+none)</td>\n",
              "      <td>0.895858</td>\n",
              "      <td>0.600935</td>\n",
              "      <td>0.451613</td>\n",
              "      <td>0.428139</td>\n",
              "      <td>0.054762</td>\n",
              "      <td>0.035908</td>\n",
              "      <td>0.125</td>\n",
              "      <td>(0.8353628851254846, 0.9427035583103766)</td>\n",
              "      <td>(0.42386574714055747, 0.74843040541238)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 Model       AUC     AUPRC        F1  \\\n",
              "0            Baseline(multi-seed mean)  0.880917  0.581821  0.428205   \n",
              "1   Best single (G5_less_dropout+none)  0.881095  0.572910  0.438102   \n",
              "2  SeedEnsemble (G5_less_dropout+none)  0.895858  0.600935  0.451613   \n",
              "\n",
              "        MCC     Brier       ECE  Threshold  \\\n",
              "0  0.388836  0.057129  0.031307      0.170   \n",
              "1  0.393739  0.057830  0.032222      0.175   \n",
              "2  0.428139  0.054762  0.035908      0.125   \n",
              "\n",
              "                                   AUC_CI95  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2  (0.8353628851254846, 0.9427035583103766)   \n",
              "\n",
              "                                AUPRC_CI95  \n",
              "0                                      NaN  \n",
              "1                                      NaN  \n",
              "2  (0.42386574714055747, 0.74843040541238)  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Exp8] 结论：当前AUC=0.8959，距0.952还差0.0561。在现有数据+ProteinBERT约束下，存在明显性能天花板。\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================\n",
        "# 实验8：同构种子集成 + 上限判断（阶段5/6）\n",
        "# 目标：只用同一ProteinBERT流程做seed ensemble，并给出是否接近0.952的现实结论\n",
        "# =====================================================================\n",
        "\n",
        "best_cfg = [c for c in finetune_cfgs if c['name'] == BEST_CALIB_CONFIG][0]\n",
        "\n",
        "def apply_calibration(method, yv_true, yv_prob, yt_prob):\n",
        "    if method == 'none':\n",
        "        return yv_prob, yt_prob\n",
        "    if method == 'platt':\n",
        "        clf = LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42)\n",
        "        clf.fit(yv_prob.reshape(-1, 1), yv_true.astype(int))\n",
        "        return (\n",
        "            clf.predict_proba(yv_prob.reshape(-1, 1))[:, 1],\n",
        "            clf.predict_proba(yt_prob.reshape(-1, 1))[:, 1],\n",
        "        )\n",
        "\n",
        "    iso = IsotonicRegression(out_of_bounds='clip')\n",
        "    iso.fit(yv_prob, yv_true.astype(int))\n",
        "    return iso.predict(yv_prob), iso.predict(yt_prob)\n",
        "\n",
        "single_rows = []\n",
        "all_valid_probs = []\n",
        "all_valid_true = []\n",
        "all_test_probs = []\n",
        "all_test_true = None\n",
        "\n",
        "for seed in SEEDS:\n",
        "    tr_df, va_df = train_test_split(\n",
        "        full_train, test_size=0.1, stratify=full_train['label'], random_state=seed\n",
        "    )\n",
        "    mg, ie, _, (yt_true, yt_prob) = run_finetune_once(\n",
        "        tr_df.reset_index(drop=True), va_df.reset_index(drop=True), full_test, best_cfg\n",
        "    )\n",
        "    yv_true, yv_prob = predict_proteinbert_probs(mg, ie, va_df['seq'], va_df['label'])\n",
        "\n",
        "    calib_valid, calib_test = apply_calibration(BEST_CALIB_METHOD, yv_true, yv_prob, yt_prob)\n",
        "    thr_seed, _ = select_best_threshold(yv_true, calib_valid)\n",
        "    met_seed = summarize_metrics(yt_true, calib_test, thr_seed)\n",
        "    single_rows.append({'Seed': seed, **met_seed})\n",
        "\n",
        "    all_valid_probs.append(calib_valid)\n",
        "    all_valid_true.append(yv_true)\n",
        "    all_test_probs.append(calib_test)\n",
        "    if all_test_true is None:\n",
        "        all_test_true = yt_true\n",
        "\n",
        "single_df = pd.DataFrame(single_rows)\n",
        "\n",
        "# 集成阈值仅在验证集（跨seed拼接）选择，测试集只评一次\n",
        "pool_valid_true = np.concatenate(all_valid_true)\n",
        "pool_valid_prob = np.concatenate(all_valid_probs)\n",
        "ens_thr, _ = select_best_threshold(pool_valid_true, pool_valid_prob)\n",
        "ens_prob = np.mean(np.vstack(all_test_probs), axis=0)\n",
        "ens_met = summarize_metrics(all_test_true, ens_prob, ens_thr)\n",
        "\n",
        "auc_ci = bootstrap_ci(all_test_true, ens_prob, roc_auc_score, n_boot=500)\n",
        "auprc_ci = bootstrap_ci(all_test_true, ens_prob, average_precision_score, n_boot=500)\n",
        "\n",
        "res = pd.DataFrame([\n",
        "    {\n",
        "        'Model': 'Baseline(multi-seed mean)',\n",
        "        'AUC': float(BASELINE_RESULT['AUC'].mean()),\n",
        "        'AUPRC': float(BASELINE_RESULT['AUPRC'].mean()),\n",
        "        'F1': float(BASELINE_RESULT['F1'].mean()),\n",
        "        'MCC': float(BASELINE_RESULT['MCC'].mean()),\n",
        "        'Brier': float(BASELINE_RESULT['Brier'].mean()),\n",
        "        'ECE': float(BASELINE_RESULT['ECE'].mean()),\n",
        "        'Threshold': float(BASELINE_RESULT['Threshold'].mean()),\n",
        "        'AUC_CI95': np.nan,\n",
        "        'AUPRC_CI95': np.nan,\n",
        "    },\n",
        "    {\n",
        "        'Model': f'Best single ({BEST_CALIB_CONFIG}+{BEST_CALIB_METHOD})',\n",
        "        'AUC': float(single_df['AUC'].mean()),\n",
        "        'AUPRC': float(single_df['AUPRC'].mean()),\n",
        "        'F1': float(single_df['F1'].mean()),\n",
        "        'MCC': float(single_df['MCC'].mean()),\n",
        "        'Brier': float(single_df['Brier'].mean()),\n",
        "        'ECE': float(single_df['ECE'].mean()),\n",
        "        'Threshold': float(single_df['Threshold'].mean()),\n",
        "        'AUC_CI95': np.nan,\n",
        "        'AUPRC_CI95': np.nan,\n",
        "    },\n",
        "    {\n",
        "        'Model': f'SeedEnsemble ({BEST_CALIB_CONFIG}+{BEST_CALIB_METHOD})',\n",
        "        **ens_met,\n",
        "        'AUC_CI95': auc_ci,\n",
        "        'AUPRC_CI95': auprc_ci,\n",
        "    },\n",
        "])\n",
        "\n",
        "print('[Exp8] 最终结果对照:')\n",
        "display(res[['Model','AUC','AUPRC','F1','MCC','Brier','ECE','Threshold','AUC_CI95','AUPRC_CI95']])\n",
        "\n",
        "target_auc = 0.952\n",
        "gap = target_auc - float(ens_met['AUC'])\n",
        "if ens_met['AUC'] >= target_auc:\n",
        "    print(f'[Exp8] 结论：已达到目标AUC {target_auc:.3f}。')\n",
        "elif ens_met['AUC'] >= 0.92:\n",
        "    print(f'[Exp8] 结论：已逼近目标，当前AUC={ens_met[\"AUC\"]:.4f}，距{target_auc:.3f}还差{gap:.4f}。')\n",
        "else:\n",
        "    print(f'[Exp8] 结论：当前AUC={ens_met[\"AUC\"]:.4f}，距{target_auc:.3f}还差{gap:.4f}。在现有数据+ProteinBERT约束下，存在明显性能天花板。')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf24pb",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
